{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20690a6f",
   "metadata": {},
   "source": [
    "# HW8\n",
    "\n",
    "# Student Name: Hung Yi-Le\n",
    "\n",
    "# GITHUB: ZackLa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0dc0061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54d336e",
   "metadata": {},
   "source": [
    "# 1. Supervised, Semi-Supervised, and Unsupervised Learning (a) Download the Breast Cancer Wisconsin (Diagnostic) Data Set from: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+ %28Diagnostic%29. Download the data in https://archive.ics.uci.edu/ml/ machine-learning-databases/breast-cancer-wisconsin/wdbc.data, which has IDs, classes (Benign=B, Malignant=M), and 30 attributes. This data has two output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c3528f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('../data/wdbc.data', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f355123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0  1      2      3       4       5        6        7        8   \\\n",
       "0      842302  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.30010   \n",
       "1      842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.08690   \n",
       "2    84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.19740   \n",
       "3    84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.24140   \n",
       "4    84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800   \n",
       "..        ... ..    ...    ...     ...     ...      ...      ...      ...   \n",
       "564    926424  M  21.56  22.39  142.00  1479.0  0.11100  0.11590  0.24390   \n",
       "565    926682  M  20.13  28.25  131.20  1261.0  0.09780  0.10340  0.14400   \n",
       "566    926954  M  16.60  28.08  108.30   858.1  0.08455  0.10230  0.09251   \n",
       "567    927241  M  20.60  29.33  140.10  1265.0  0.11780  0.27700  0.35140   \n",
       "568     92751  B   7.76  24.54   47.92   181.0  0.05263  0.04362  0.00000   \n",
       "\n",
       "          9   ...      22     23      24      25       26       27      28  \\\n",
       "0    0.14710  ...  25.380  17.33  184.60  2019.0  0.16220  0.66560  0.7119   \n",
       "1    0.07017  ...  24.990  23.41  158.80  1956.0  0.12380  0.18660  0.2416   \n",
       "2    0.12790  ...  23.570  25.53  152.50  1709.0  0.14440  0.42450  0.4504   \n",
       "3    0.10520  ...  14.910  26.50   98.87   567.7  0.20980  0.86630  0.6869   \n",
       "4    0.10430  ...  22.540  16.67  152.20  1575.0  0.13740  0.20500  0.4000   \n",
       "..       ...  ...     ...    ...     ...     ...      ...      ...     ...   \n",
       "564  0.13890  ...  25.450  26.40  166.10  2027.0  0.14100  0.21130  0.4107   \n",
       "565  0.09791  ...  23.690  38.25  155.00  1731.0  0.11660  0.19220  0.3215   \n",
       "566  0.05302  ...  18.980  34.12  126.70  1124.0  0.11390  0.30940  0.3403   \n",
       "567  0.15200  ...  25.740  39.42  184.60  1821.0  0.16500  0.86810  0.9387   \n",
       "568  0.00000  ...   9.456  30.37   59.16   268.6  0.08996  0.06444  0.0000   \n",
       "\n",
       "         29      30       31  \n",
       "0    0.2654  0.4601  0.11890  \n",
       "1    0.1860  0.2750  0.08902  \n",
       "2    0.2430  0.3613  0.08758  \n",
       "3    0.2575  0.6638  0.17300  \n",
       "4    0.1625  0.2364  0.07678  \n",
       "..      ...     ...      ...  \n",
       "564  0.2216  0.2060  0.07115  \n",
       "565  0.1628  0.2572  0.06637  \n",
       "566  0.1418  0.2218  0.07820  \n",
       "567  0.2650  0.4087  0.12400  \n",
       "568  0.0000  0.2871  0.07039  \n",
       "\n",
       "[569 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c1ac76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = d[[0,1]]\n",
    "d1 = d1.rename(columns = {0: \"ID number\", 1: \"Diagnosis\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7aa96a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d.drop(columns = [0,1])\n",
    "## reset the index\n",
    "d.columns = range(1, d.shape[1]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "255bb95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04938ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame(scaler.transform(d))\n",
    "d.columns = range(1, d.shape[1]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee18c1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.521037</td>\n",
       "      <td>0.022658</td>\n",
       "      <td>0.545989</td>\n",
       "      <td>0.363733</td>\n",
       "      <td>0.593753</td>\n",
       "      <td>0.792037</td>\n",
       "      <td>0.703140</td>\n",
       "      <td>0.731113</td>\n",
       "      <td>0.686364</td>\n",
       "      <td>0.605518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620776</td>\n",
       "      <td>0.141525</td>\n",
       "      <td>0.668310</td>\n",
       "      <td>0.450698</td>\n",
       "      <td>0.601136</td>\n",
       "      <td>0.619292</td>\n",
       "      <td>0.568610</td>\n",
       "      <td>0.912027</td>\n",
       "      <td>0.598462</td>\n",
       "      <td>0.418864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.643144</td>\n",
       "      <td>0.272574</td>\n",
       "      <td>0.615783</td>\n",
       "      <td>0.501591</td>\n",
       "      <td>0.289880</td>\n",
       "      <td>0.181768</td>\n",
       "      <td>0.203608</td>\n",
       "      <td>0.348757</td>\n",
       "      <td>0.379798</td>\n",
       "      <td>0.141323</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606901</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>0.539818</td>\n",
       "      <td>0.435214</td>\n",
       "      <td>0.347553</td>\n",
       "      <td>0.154563</td>\n",
       "      <td>0.192971</td>\n",
       "      <td>0.639175</td>\n",
       "      <td>0.233590</td>\n",
       "      <td>0.222878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.601496</td>\n",
       "      <td>0.390260</td>\n",
       "      <td>0.595743</td>\n",
       "      <td>0.449417</td>\n",
       "      <td>0.514309</td>\n",
       "      <td>0.431017</td>\n",
       "      <td>0.462512</td>\n",
       "      <td>0.635686</td>\n",
       "      <td>0.509596</td>\n",
       "      <td>0.211247</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556386</td>\n",
       "      <td>0.360075</td>\n",
       "      <td>0.508442</td>\n",
       "      <td>0.374508</td>\n",
       "      <td>0.483590</td>\n",
       "      <td>0.385375</td>\n",
       "      <td>0.359744</td>\n",
       "      <td>0.835052</td>\n",
       "      <td>0.403706</td>\n",
       "      <td>0.213433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.210090</td>\n",
       "      <td>0.360839</td>\n",
       "      <td>0.233501</td>\n",
       "      <td>0.102906</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.811361</td>\n",
       "      <td>0.565604</td>\n",
       "      <td>0.522863</td>\n",
       "      <td>0.776263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248310</td>\n",
       "      <td>0.385928</td>\n",
       "      <td>0.241347</td>\n",
       "      <td>0.094008</td>\n",
       "      <td>0.915472</td>\n",
       "      <td>0.814012</td>\n",
       "      <td>0.548642</td>\n",
       "      <td>0.884880</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.773711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.629893</td>\n",
       "      <td>0.156578</td>\n",
       "      <td>0.630986</td>\n",
       "      <td>0.489290</td>\n",
       "      <td>0.430351</td>\n",
       "      <td>0.347893</td>\n",
       "      <td>0.463918</td>\n",
       "      <td>0.518390</td>\n",
       "      <td>0.378283</td>\n",
       "      <td>0.186816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.519744</td>\n",
       "      <td>0.123934</td>\n",
       "      <td>0.506948</td>\n",
       "      <td>0.341575</td>\n",
       "      <td>0.437364</td>\n",
       "      <td>0.172415</td>\n",
       "      <td>0.319489</td>\n",
       "      <td>0.558419</td>\n",
       "      <td>0.157500</td>\n",
       "      <td>0.142595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.428813</td>\n",
       "      <td>0.678668</td>\n",
       "      <td>0.566490</td>\n",
       "      <td>0.526948</td>\n",
       "      <td>0.296055</td>\n",
       "      <td>0.571462</td>\n",
       "      <td>0.690358</td>\n",
       "      <td>0.336364</td>\n",
       "      <td>0.132056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.623266</td>\n",
       "      <td>0.383262</td>\n",
       "      <td>0.576174</td>\n",
       "      <td>0.452664</td>\n",
       "      <td>0.461137</td>\n",
       "      <td>0.178527</td>\n",
       "      <td>0.328035</td>\n",
       "      <td>0.761512</td>\n",
       "      <td>0.097575</td>\n",
       "      <td>0.105667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>0.622320</td>\n",
       "      <td>0.626987</td>\n",
       "      <td>0.604036</td>\n",
       "      <td>0.474019</td>\n",
       "      <td>0.407782</td>\n",
       "      <td>0.257714</td>\n",
       "      <td>0.337395</td>\n",
       "      <td>0.486630</td>\n",
       "      <td>0.349495</td>\n",
       "      <td>0.113100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560655</td>\n",
       "      <td>0.699094</td>\n",
       "      <td>0.520892</td>\n",
       "      <td>0.379915</td>\n",
       "      <td>0.300007</td>\n",
       "      <td>0.159997</td>\n",
       "      <td>0.256789</td>\n",
       "      <td>0.559450</td>\n",
       "      <td>0.198502</td>\n",
       "      <td>0.074315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0.455251</td>\n",
       "      <td>0.621238</td>\n",
       "      <td>0.445788</td>\n",
       "      <td>0.303118</td>\n",
       "      <td>0.288165</td>\n",
       "      <td>0.254340</td>\n",
       "      <td>0.216753</td>\n",
       "      <td>0.263519</td>\n",
       "      <td>0.267677</td>\n",
       "      <td>0.137321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.393099</td>\n",
       "      <td>0.589019</td>\n",
       "      <td>0.379949</td>\n",
       "      <td>0.230731</td>\n",
       "      <td>0.282177</td>\n",
       "      <td>0.273705</td>\n",
       "      <td>0.271805</td>\n",
       "      <td>0.487285</td>\n",
       "      <td>0.128721</td>\n",
       "      <td>0.151909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>0.644564</td>\n",
       "      <td>0.663510</td>\n",
       "      <td>0.665538</td>\n",
       "      <td>0.475716</td>\n",
       "      <td>0.588336</td>\n",
       "      <td>0.790197</td>\n",
       "      <td>0.823336</td>\n",
       "      <td>0.755467</td>\n",
       "      <td>0.675253</td>\n",
       "      <td>0.425442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.633582</td>\n",
       "      <td>0.730277</td>\n",
       "      <td>0.668310</td>\n",
       "      <td>0.402035</td>\n",
       "      <td>0.619626</td>\n",
       "      <td>0.815758</td>\n",
       "      <td>0.749760</td>\n",
       "      <td>0.910653</td>\n",
       "      <td>0.497142</td>\n",
       "      <td>0.452315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>0.036869</td>\n",
       "      <td>0.501522</td>\n",
       "      <td>0.028540</td>\n",
       "      <td>0.015907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266162</td>\n",
       "      <td>0.187026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054287</td>\n",
       "      <td>0.489072</td>\n",
       "      <td>0.043578</td>\n",
       "      <td>0.020497</td>\n",
       "      <td>0.124084</td>\n",
       "      <td>0.036043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.257441</td>\n",
       "      <td>0.100682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           1         2         3         4         5         6         7   \\\n",
       "0    0.521037  0.022658  0.545989  0.363733  0.593753  0.792037  0.703140   \n",
       "1    0.643144  0.272574  0.615783  0.501591  0.289880  0.181768  0.203608   \n",
       "2    0.601496  0.390260  0.595743  0.449417  0.514309  0.431017  0.462512   \n",
       "3    0.210090  0.360839  0.233501  0.102906  0.811321  0.811361  0.565604   \n",
       "4    0.629893  0.156578  0.630986  0.489290  0.430351  0.347893  0.463918   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "564  0.690000  0.428813  0.678668  0.566490  0.526948  0.296055  0.571462   \n",
       "565  0.622320  0.626987  0.604036  0.474019  0.407782  0.257714  0.337395   \n",
       "566  0.455251  0.621238  0.445788  0.303118  0.288165  0.254340  0.216753   \n",
       "567  0.644564  0.663510  0.665538  0.475716  0.588336  0.790197  0.823336   \n",
       "568  0.036869  0.501522  0.028540  0.015907  0.000000  0.074351  0.000000   \n",
       "\n",
       "           8         9         10  ...        21        22        23  \\\n",
       "0    0.731113  0.686364  0.605518  ...  0.620776  0.141525  0.668310   \n",
       "1    0.348757  0.379798  0.141323  ...  0.606901  0.303571  0.539818   \n",
       "2    0.635686  0.509596  0.211247  ...  0.556386  0.360075  0.508442   \n",
       "3    0.522863  0.776263  1.000000  ...  0.248310  0.385928  0.241347   \n",
       "4    0.518390  0.378283  0.186816  ...  0.519744  0.123934  0.506948   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "564  0.690358  0.336364  0.132056  ...  0.623266  0.383262  0.576174   \n",
       "565  0.486630  0.349495  0.113100  ...  0.560655  0.699094  0.520892   \n",
       "566  0.263519  0.267677  0.137321  ...  0.393099  0.589019  0.379949   \n",
       "567  0.755467  0.675253  0.425442  ...  0.633582  0.730277  0.668310   \n",
       "568  0.000000  0.266162  0.187026  ...  0.054287  0.489072  0.043578   \n",
       "\n",
       "           24        25        26        27        28        29        30  \n",
       "0    0.450698  0.601136  0.619292  0.568610  0.912027  0.598462  0.418864  \n",
       "1    0.435214  0.347553  0.154563  0.192971  0.639175  0.233590  0.222878  \n",
       "2    0.374508  0.483590  0.385375  0.359744  0.835052  0.403706  0.213433  \n",
       "3    0.094008  0.915472  0.814012  0.548642  0.884880  1.000000  0.773711  \n",
       "4    0.341575  0.437364  0.172415  0.319489  0.558419  0.157500  0.142595  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "564  0.452664  0.461137  0.178527  0.328035  0.761512  0.097575  0.105667  \n",
       "565  0.379915  0.300007  0.159997  0.256789  0.559450  0.198502  0.074315  \n",
       "566  0.230731  0.282177  0.273705  0.271805  0.487285  0.128721  0.151909  \n",
       "567  0.402035  0.619626  0.815758  0.749760  0.910653  0.497142  0.452315  \n",
       "568  0.020497  0.124084  0.036043  0.000000  0.000000  0.257441  0.100682  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1f9b895",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([d1, d], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1750d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28f999ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID number</th>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>0.521037</td>\n",
       "      <td>0.022658</td>\n",
       "      <td>0.545989</td>\n",
       "      <td>0.363733</td>\n",
       "      <td>0.593753</td>\n",
       "      <td>0.792037</td>\n",
       "      <td>0.703140</td>\n",
       "      <td>0.731113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620776</td>\n",
       "      <td>0.141525</td>\n",
       "      <td>0.668310</td>\n",
       "      <td>0.450698</td>\n",
       "      <td>0.601136</td>\n",
       "      <td>0.619292</td>\n",
       "      <td>0.568610</td>\n",
       "      <td>0.912027</td>\n",
       "      <td>0.598462</td>\n",
       "      <td>0.418864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>0.643144</td>\n",
       "      <td>0.272574</td>\n",
       "      <td>0.615783</td>\n",
       "      <td>0.501591</td>\n",
       "      <td>0.289880</td>\n",
       "      <td>0.181768</td>\n",
       "      <td>0.203608</td>\n",
       "      <td>0.348757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606901</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>0.539818</td>\n",
       "      <td>0.435214</td>\n",
       "      <td>0.347553</td>\n",
       "      <td>0.154563</td>\n",
       "      <td>0.192971</td>\n",
       "      <td>0.639175</td>\n",
       "      <td>0.233590</td>\n",
       "      <td>0.222878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>0.601496</td>\n",
       "      <td>0.390260</td>\n",
       "      <td>0.595743</td>\n",
       "      <td>0.449417</td>\n",
       "      <td>0.514309</td>\n",
       "      <td>0.431017</td>\n",
       "      <td>0.462512</td>\n",
       "      <td>0.635686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556386</td>\n",
       "      <td>0.360075</td>\n",
       "      <td>0.508442</td>\n",
       "      <td>0.374508</td>\n",
       "      <td>0.483590</td>\n",
       "      <td>0.385375</td>\n",
       "      <td>0.359744</td>\n",
       "      <td>0.835052</td>\n",
       "      <td>0.403706</td>\n",
       "      <td>0.213433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>0.210090</td>\n",
       "      <td>0.360839</td>\n",
       "      <td>0.233501</td>\n",
       "      <td>0.102906</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.811361</td>\n",
       "      <td>0.565604</td>\n",
       "      <td>0.522863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248310</td>\n",
       "      <td>0.385928</td>\n",
       "      <td>0.241347</td>\n",
       "      <td>0.094008</td>\n",
       "      <td>0.915472</td>\n",
       "      <td>0.814012</td>\n",
       "      <td>0.548642</td>\n",
       "      <td>0.884880</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.773711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>0.629893</td>\n",
       "      <td>0.156578</td>\n",
       "      <td>0.630986</td>\n",
       "      <td>0.489290</td>\n",
       "      <td>0.430351</td>\n",
       "      <td>0.347893</td>\n",
       "      <td>0.463918</td>\n",
       "      <td>0.518390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.519744</td>\n",
       "      <td>0.123934</td>\n",
       "      <td>0.506948</td>\n",
       "      <td>0.341575</td>\n",
       "      <td>0.437364</td>\n",
       "      <td>0.172415</td>\n",
       "      <td>0.319489</td>\n",
       "      <td>0.558419</td>\n",
       "      <td>0.157500</td>\n",
       "      <td>0.142595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.428813</td>\n",
       "      <td>0.678668</td>\n",
       "      <td>0.566490</td>\n",
       "      <td>0.526948</td>\n",
       "      <td>0.296055</td>\n",
       "      <td>0.571462</td>\n",
       "      <td>0.690358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.623266</td>\n",
       "      <td>0.383262</td>\n",
       "      <td>0.576174</td>\n",
       "      <td>0.452664</td>\n",
       "      <td>0.461137</td>\n",
       "      <td>0.178527</td>\n",
       "      <td>0.328035</td>\n",
       "      <td>0.761512</td>\n",
       "      <td>0.097575</td>\n",
       "      <td>0.105667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>0.622320</td>\n",
       "      <td>0.626987</td>\n",
       "      <td>0.604036</td>\n",
       "      <td>0.474019</td>\n",
       "      <td>0.407782</td>\n",
       "      <td>0.257714</td>\n",
       "      <td>0.337395</td>\n",
       "      <td>0.486630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560655</td>\n",
       "      <td>0.699094</td>\n",
       "      <td>0.520892</td>\n",
       "      <td>0.379915</td>\n",
       "      <td>0.300007</td>\n",
       "      <td>0.159997</td>\n",
       "      <td>0.256789</td>\n",
       "      <td>0.559450</td>\n",
       "      <td>0.198502</td>\n",
       "      <td>0.074315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>0.455251</td>\n",
       "      <td>0.621238</td>\n",
       "      <td>0.445788</td>\n",
       "      <td>0.303118</td>\n",
       "      <td>0.288165</td>\n",
       "      <td>0.254340</td>\n",
       "      <td>0.216753</td>\n",
       "      <td>0.263519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.393099</td>\n",
       "      <td>0.589019</td>\n",
       "      <td>0.379949</td>\n",
       "      <td>0.230731</td>\n",
       "      <td>0.282177</td>\n",
       "      <td>0.273705</td>\n",
       "      <td>0.271805</td>\n",
       "      <td>0.487285</td>\n",
       "      <td>0.128721</td>\n",
       "      <td>0.151909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>0.644564</td>\n",
       "      <td>0.663510</td>\n",
       "      <td>0.665538</td>\n",
       "      <td>0.475716</td>\n",
       "      <td>0.588336</td>\n",
       "      <td>0.790197</td>\n",
       "      <td>0.823336</td>\n",
       "      <td>0.755467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.633582</td>\n",
       "      <td>0.730277</td>\n",
       "      <td>0.668310</td>\n",
       "      <td>0.402035</td>\n",
       "      <td>0.619626</td>\n",
       "      <td>0.815758</td>\n",
       "      <td>0.749760</td>\n",
       "      <td>0.910653</td>\n",
       "      <td>0.497142</td>\n",
       "      <td>0.452315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>0.036869</td>\n",
       "      <td>0.501522</td>\n",
       "      <td>0.028540</td>\n",
       "      <td>0.015907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054287</td>\n",
       "      <td>0.489072</td>\n",
       "      <td>0.043578</td>\n",
       "      <td>0.020497</td>\n",
       "      <td>0.124084</td>\n",
       "      <td>0.036043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.257441</td>\n",
       "      <td>0.100682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID number Diagnosis         1         2         3         4         5  \\\n",
       "0       842302         M  0.521037  0.022658  0.545989  0.363733  0.593753   \n",
       "1       842517         M  0.643144  0.272574  0.615783  0.501591  0.289880   \n",
       "2     84300903         M  0.601496  0.390260  0.595743  0.449417  0.514309   \n",
       "3     84348301         M  0.210090  0.360839  0.233501  0.102906  0.811321   \n",
       "4     84358402         M  0.629893  0.156578  0.630986  0.489290  0.430351   \n",
       "..         ...       ...       ...       ...       ...       ...       ...   \n",
       "564     926424         M  0.690000  0.428813  0.678668  0.566490  0.526948   \n",
       "565     926682         M  0.622320  0.626987  0.604036  0.474019  0.407782   \n",
       "566     926954         M  0.455251  0.621238  0.445788  0.303118  0.288165   \n",
       "567     927241         M  0.644564  0.663510  0.665538  0.475716  0.588336   \n",
       "568      92751         B  0.036869  0.501522  0.028540  0.015907  0.000000   \n",
       "\n",
       "            6         7         8  ...        21        22        23  \\\n",
       "0    0.792037  0.703140  0.731113  ...  0.620776  0.141525  0.668310   \n",
       "1    0.181768  0.203608  0.348757  ...  0.606901  0.303571  0.539818   \n",
       "2    0.431017  0.462512  0.635686  ...  0.556386  0.360075  0.508442   \n",
       "3    0.811361  0.565604  0.522863  ...  0.248310  0.385928  0.241347   \n",
       "4    0.347893  0.463918  0.518390  ...  0.519744  0.123934  0.506948   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "564  0.296055  0.571462  0.690358  ...  0.623266  0.383262  0.576174   \n",
       "565  0.257714  0.337395  0.486630  ...  0.560655  0.699094  0.520892   \n",
       "566  0.254340  0.216753  0.263519  ...  0.393099  0.589019  0.379949   \n",
       "567  0.790197  0.823336  0.755467  ...  0.633582  0.730277  0.668310   \n",
       "568  0.074351  0.000000  0.000000  ...  0.054287  0.489072  0.043578   \n",
       "\n",
       "           24        25        26        27        28        29        30  \n",
       "0    0.450698  0.601136  0.619292  0.568610  0.912027  0.598462  0.418864  \n",
       "1    0.435214  0.347553  0.154563  0.192971  0.639175  0.233590  0.222878  \n",
       "2    0.374508  0.483590  0.385375  0.359744  0.835052  0.403706  0.213433  \n",
       "3    0.094008  0.915472  0.814012  0.548642  0.884880  1.000000  0.773711  \n",
       "4    0.341575  0.437364  0.172415  0.319489  0.558419  0.157500  0.142595  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "564  0.452664  0.461137  0.178527  0.328035  0.761512  0.097575  0.105667  \n",
       "565  0.379915  0.300007  0.159997  0.256789  0.559450  0.198502  0.074315  \n",
       "566  0.230731  0.282177  0.273705  0.271805  0.487285  0.128721  0.151909  \n",
       "567  0.402035  0.619626  0.815758  0.749760  0.910653  0.497142  0.452315  \n",
       "568  0.020497  0.124084  0.036043  0.000000  0.000000  0.257441  0.100682  \n",
       "\n",
       "[569 rows x 32 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ba426a",
   "metadata": {},
   "source": [
    "# (b) Monte-Carlo Simulation: Repeat the following procedures for supervised, unsupervised, and semi-supervised learning M = 30 times, and use randomly selected train and test data (make sure you use 20% of both the positve and negative classes as the test set). Then compare the average scores (accuracy, precision, recall, F1-score, and AUC) that you obtain from each algorithm. \n",
    "\n",
    "## i. Supervised Learning: Train an L1-penalized SVM to classify the data. Use 5 fold cross validation to choose the penalty parameter. Use normalized data. Report the average accuracy, precision, recall, F1-score, and AUC, for both training and test sets over your M runs. Plot the ROC and report the confusion matrix for training and testing in one of the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ff62275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb859fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = df.loc[df['Diagnosis'] == 'B']\n",
    "positive = df.loc[df['Diagnosis'] == 'M']\n",
    "#\n",
    "#negative = negative.sample(frac=0.2, axis = 'rows', random_state = 42)\n",
    "#positive = positive.sample(frac=0.2, axis = 'rows', random_state = 42)\n",
    "\n",
    "## select test set\n",
    "#test = pd.concat([positive, negative])\n",
    "#train = pd.concat([df, test, test]).drop_duplicates(keep = False)\n",
    "#positive.sample(frac=0.2, axis = 'rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01126ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train = []\n",
    "precision_train = []\n",
    "recall_train = []\n",
    "f1_score_train = []\n",
    "auc_train = []\n",
    "\n",
    "accuracy_test = []\n",
    "precision_test = []\n",
    "recall_test = []\n",
    "f1_score_test = []\n",
    "auc_test = []\n",
    "\n",
    "r = []\n",
    "\n",
    "for i in list(range(1,31,1)):\n",
    "    negative = df.loc[df['Diagnosis'] == 'B']\n",
    "    positive = df.loc[df['Diagnosis'] == 'M']\n",
    "\n",
    "    negative = negative.sample(frac=0.2, axis = 'rows', random_state = i)\n",
    "    positive = positive.sample(frac=0.2, axis = 'rows', random_state = i)\n",
    "\n",
    "    ## select test set\n",
    "    test = pd.concat([positive, negative])\n",
    "    train = pd.concat([df, test, test]).drop_duplicates(keep = False)\n",
    "\n",
    "    y_test = test['Diagnosis']\n",
    "    y_train = train['Diagnosis']\n",
    "    x_test = test.loc[:,1:]\n",
    "    x_train = train.loc[:,1:]\n",
    "\n",
    "    y_test = y_test.replace([\"B\", \"M\"], [0, 1])\n",
    "    y_train = y_train.replace([\"B\", \"M\"], [0, 1])\n",
    "\n",
    "    model = svm.LinearSVC(penalty = 'l1', dual = False, max_iter = 300000)\n",
    "    parameters = {'C': [0.01, 0.1, 10, 100, 1000, 10000]}\n",
    "    clf = GridSearchCV(model, parameters, scoring = 'accuracy', cv = StratifiedKFold(n_splits = 5))\n",
    "    \n",
    "    # for test set\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    \n",
    "    #for AUC (test)\n",
    "    y_score = clf.decision_function(x_test)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "    auc1 = auc(fpr, tpr)\n",
    "\n",
    "    accuracy_test.append(clf.score(x_test, y_test))\n",
    "    precision_test.append(precision_score(y_test, y_pred))\n",
    "    recall_test.append(recall_score(y_test, y_pred))\n",
    "    f1_score_test.append(f1_score(y_test, y_pred))\n",
    "    auc_test.append(auc1)\n",
    "    \n",
    "    #for train set\n",
    "    y_pred00 = clf.predict(x_train)\n",
    "    #for AUC (train)\n",
    "    y_score = clf.decision_function(x_train)\n",
    "    fpr1, tpr1, thresholds1 = roc_curve(y_train, y_score)\n",
    "    auc2 = auc(fpr1, tpr1)\n",
    "    \n",
    "    accuracy_train.append(clf.score(x_train, y_train))\n",
    "    precision_train.append(precision_score(y_train, y_pred00))\n",
    "    recall_train.append(recall_score(y_train, y_pred00))\n",
    "    f1_score_train.append(f1_score(y_train, y_pred00))\n",
    "    auc_train.append(auc2)\n",
    "    r.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "653acd39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ROC for test set\n",
    "plt.plot(fpr, tpr, color = \"Blue\")\n",
    "plt.plot([0,1],[0,1], color = 'Red', linestyle = \"--\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01e9937",
   "metadata": {},
   "source": [
    "### The ROC graph for test set is shown as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa333256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0   1\n",
       "True             \n",
       "0          70   4\n",
       "1           1  38"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_confusion = pd.DataFrame(confusion_matrix(y_pred, y_test))\n",
    "df_confusion = df_confusion.rename_axis(index = 'True', columns = 'Predicted')\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7367e9df",
   "metadata": {},
   "source": [
    "### The confusion marix for test set data is shown as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00009e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEGCAYAAAB7DNKzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAgGUlEQVR4nO3deXhV1bnH8e8LBCqCxiJwFVCQQaQiiAy2V2twBLRQhwo4VuzlOuBQJ3CqUytaWweQSpFia61ii6CoiPhUA1oHBEEFEURQQKyIiowiIe/9Y4WbNCaHQ3L22Wf4fZ4nD9n77BxflnB+rLX2XsvcHRERkerUibsAERHJbAoKERFJSEEhIiIJKShERCQhBYWIiCRUL+4CdlVhYaG3a9cu7jIywqZNm9h9993jLiMjqC3KqS3KqS3KzZ07d627N63Jz2ZdUDRv3pw5c+bEXUZGKC4upqioKO4yMoLaopzaopzaopyZfVzTn9XQk4iIJKSgEBGRhBQUIiKSkIJCREQSUlCIiEhCCgoREUlIQSEiIgkpKEREJCEFhYiIJKSgEBGRhBQUIiKSkIJCREQSUlCIiEhCCgoREUlIQSEiIglFFhRmNsHM1pjZgmpeNzMbZWZLzewdM+sWVS0iIlJzUfYo/gz0SfB6X6B92ddQ4IEIaxERkRqKbIc7d59lZq0TXDIAeNjdHXjdzArNbB93/zSqmjLJuHHw6KO1e49167pSWJiScrKe2qKc2qKc2iI14twKtQWwssLxqrJz3wkKMxtK6HXQtGlTiouLE77x00/vwz//2TxlhUbh7bcLAejSZV2N32P79u2sW1fzn88laotyaoty+d4Wh294iVLqMLvxUbV6nziDwqo451Vd6O7jgHEA//VfB/vNNxclfOOZM8OvR9WubSJ11FFwxhkwdGhhjd9D+wGXU1uUU1uUy9u2+OgjuOIKmDIFjj8enh+AVfWJm6Q4g2IV0KrCcUtg9c5+aP36AubPh65dq7+m/EO4lhWKiGSTLVvgzjvDV5068JvfhMCopTiDYiowzMwmAr2Ar5OZn9iypS49e8JORp9ERPLPlClwyy0wcCDcdRe0arXzn0lCZEFhZo8BRcDeZrYKuAkoAHD3scA0oB+wFNgMnJfse59xRqqrFRHJUosWwZIlMGAADBoE7dpBz54p/U9EedfT4J287sDFu/q+u+22XUNKIiLr18Ott8J990HLlnDiiVCvXspDAvRktohIdikthYcfhgMPhLvvhp//HGbPDiERkTjnKEREZFfNnQvnngu9esHUqdCjR+T/SfUoREQy3dq18Pe/h+979IAXX4RXX01LSICCQkQkc5WUwJgx0KFD6EWsWRPO9+4dbn9NEwWFiEgmevll6N4dhg2DQw+FOXOgWbNYStEchYhIpvnsMzj2WGjeHP7xDzj1VGr1aHUtqUchIpIJtm4NoQAhIJ55JjwjcdppsYYEKChEROI3fTp07gynnx6GmACOOw523z3eusooKERE4rJsWXiium/f0Gt47rkwL5FhNEchIhKHbdvCCqZffRUW8bv8cqhfP+6qqqSgEBFJF/fQazjhBCgoCE9Yd+gALVrEXVlCGnoSEUmHhQvDnUwnngiPPRbO9e6d8SEBCgoRkWitWxeGlbp0gXnz4P77wyqvWURDTyIiUTr55LDt5v/8T9hIaO+9465olykoRERSbc6csLpr48YwcmSYjzjssLirqjENPYmIpMrnn8MvfhH2hPjd78K5ww/P6pAABYWISO2VlMCoUdC+PfzlL2Gf6iuvjLuqlNHQk4hIbV16KTzwQLiradQoOOiguCtKKQWFiEhNrFwZlvpu0SIExbHHhonrmNdlioKGnkREdsXWrXD77dCxI1x1VTjXsSOcckpOhgSoRyEikrxnn4XLLoMPPwy9h5Ej464oLdSjEBFJxpgxcNJJ4VbXGTNg8mRo3TruqtJCPQoRkeps2hS2H23TBgYODAv5XXRRxi7eFxX1KEREKnOHiRPDQ3MDB4bjvffO6BVeo6SgEBGp6J13wmJ9gweHParvuSdnJ6mTpaEnEZEdXngB+vSBwkIYOzY8ZV23btxVxU49ChHJb6WlsHx5+P7HP4brr4clS+B//1chUUZBISL56403oFevEBCbNkGDBnDrrdCkSdyVZRQFhYjkn88+gyFDwoJ9n3wCd9wBDRvGXVXG0hyFiOSXpUvDaq5btsA118ANN4TlwKVaCgoRyQ+ffBLWZWrbFi6+GM49N9z+KjsV6dCTmfUxs8VmttTMRlTx+p5m9rSZvW1mC83svCjrEZE8tGIFnH56CIVVq8KtrrffrpDYBZH1KMysLjAGOA5YBbxpZlPd/b0Kl10MvOfuPzGzpsBiM/ubu38bVV0ikie++Yb9//pXeOyxcHzttVm5DWkmiHLoqSew1N2XAZjZRGAAUDEoHGhsZgY0Ar4ESiKsSUTywcaN0LUrbT78EE47Lew2t//+cVeVtaIMihbAygrHq4Bela65H5gKrAYaAwPdvbTyG5nZUGAoQEHBwRQXF0dRb9bZuHGj2qKM2qJcPrdFwbp1bCssBGD/I4/k3+edx9b//u/wnMSOZyVkl0UZFFU98+6Vjk8A5gNHA22BF8zsZXdf/x8/5D4OGAfQsOGhXlRUlPJis1FxcTFqi0BtUS4v22LDBvj1r8Pucq+8Eu5qKiri43xsiwhEOZm9CmhV4bgloedQ0XnAZA+WAsuBjhHWJCK5xB0efTRsHPTb38KgQdCyZdxV5Zwog+JNoL2ZtTGz+sAgwjBTRSuAYwDMrDlwILAswppEJFe4Q79+cOaZsO++8Npr8NBD0Lx53JXlnMiGnty9xMyGAc8DdYEJ7r7QzC4oe30scBvwZzN7lzBUNdzd10ZVk4jkgPXrwwNyZtC3L5x6anjKuo4WmohKpA/cufs0YFqlc2MrfL8aOD7KGkQkR2zfDn/6E1x3HTz4YNiK9NJL464qLyiCRSTzvfYa9OwZVnT9wQ+gXbu4K8orCgoRyWzXXAM/+lFYyO+xx6C4GDp3jruqvKKgEJHMs21b+ALo1g1GjID33w93NeX5bnNxUFCISGZ54QU45BAYPTocDxoEI0dCo0bx1pXHFBQikhk++ijcwXT88aE30alT3BVJGQWFiMRvwgQ46CCYPh1+8xtYsCDsXS0ZQftRiEg83EPPoX596NAB+vcPi/e1arXzn5W0Uo9CRNLv/fdDj+GKK8LxEUfA448rJDKUgkJE0mf9erj66nB76xtvhOEmyXgaehKR9HjpJTjjjPA8xJAhYZe5Zs3irkqSoKAQkWht3w5164aNg9q3h6eeCk9ZS9ZQUIhINL74Aq6/HlavhqlT4YADYNasuKuSGtAchYik1vbt8MAD4U6m8eNDQJRoh+Nsph6FiKTOkiUwcCDMnw+9e4cd5w4+OO6qpJYUFCJSe+5hDaZmzaBevXCr689+pnWZcoSCQkRq7ttv4d57wwT1zJlQWAizZysgcozmKESkZqZPD89DDB8O3/8+fP11OK+QyDkKChHZNV9+CT/9adiGtLQUnn0Wnn4amjSJuzKJiIJCRJLjHn5t3Bj+/e+w9PeCBdCvX7x1SeQUFCKSmDtMmgSHHx6GlwoK4NVXw2ZCDRrEXZ2kgYJCRKr33ntw3HHhDqZvvgk9CYA6+ujIJ/q/LSLfVVISVnbt0gXmzoX77w+/Hnhg3JVJDHR7rIh8V9268MEHcN55YSOhpk3jrkhipB6FiARz54anqZctC7e4TpkC48YpJERBIZL3Pv8chg6FHj1g0aKwdzWEJ6xFUFCI5Lcdi/c99BD88peweDEcfXTcVUmG0T8ZRPLZggXQvXtYvE+7zUk11KMQySeffBJ2mfvXv8LxPffAjBkKCUlIQSGSD7ZuhTvuCLe3Tp4c5iIA6tfX2kyyUxp6Esl1M2bAsGHhdtef/hTuvhvatIm7KskikfYozKyPmS02s6VmNqKaa4rMbL6ZLTSzmVHWI5KX3nkn9BqmTw+3vCokZBdFFhRmVhcYA/QFOgGDzaxTpWsKgT8A/d39B8DPoqpHJF/U2bIl7FX997+HE5ddBu++CyecEG9hkrWi7FH0BJa6+zJ3/xaYCAyodM0ZwGR3XwHg7msirEckt7nD44/T89xz4fbb4Y03wvmCgjAXIVJDUc5RtABWVjheBfSqdE0HoMDMioHGwH3u/nDlNzKzocBQgIKCgykuLo6i3qyzceNGtUWZfG+LhsuX037UKPaaP5+tBxzAohtv5OvOnSGP2wT05yJVogyKqm6l8Cr++4cBxwC7Aa+Z2evuvuQ/fsh9HDAOoGHDQ72oqCj11Wah4uJi1BZB3rfFF1/AihXwhz8wr0MHio45Ju6KMkLe/7lIkV0eejKzumZ2ZhKXrgJaVThuCayu4prp7r7J3dcCs4Auu1qTSN4pLYUJE8KqrgCnnAIffggXXhgW9BNJoWqDwsz2MLNrzex+MzvegkuAZcDpSbz3m0B7M2tjZvWBQcDUStc8BRxpZvXMrCFhaGpRzX4rInli9mz44Q/h/PNh6tQwN2EGhYVxVyY5KlGP4q/AgcC7wC+AGcBpwAB3rzwp/R3uXgIMA54nfPj/3d0XmtkFZnZB2TWLgOnAO8BsYLy7L6jF70ckd61ZE8KhV68wzPTXv8Lzz+uBOYlcojmKA9y9M4CZjQfWAvu5+4Zk39zdpwHTKp0bW+n4LuCupCsWyVcffwyPPAJXXQU33gh77BF3RZInEgXFth3fuPt2M1u+KyEhIinw0kthXaYbbgjLgK9cCc2axV2V5JlEQ09dzGy9mW0wsw3AIRWO16erQJG8tHIlDBwYlvx+6CHYuDGcV0hIDKoNCnev6+57uHvjsq96FY7V5xWJwjffhK1HO3YME9W33BKWAm/UKO7KJI9VO/RkZt8DLgDaESabJ5RNUItIVNauDU9V9+0Lv/897L9/3BWJJBx6+gvQnXDXUz/g92mpSCTffPBBmINwh5Yt4f33YdIkhYRkjEST2Z0q3PX0J8LtqyKSKhs3hmGmu++GBg3g5z+Hdu2gVaud/qhIOiXqUVS860lDTiKp4g6PPRbmIe64AwYPhiVLQkiIZKBEPYquFe5uMmC3smMDXBPaIjW0eTNcfTU0bw7/+Ed4ylokgyUKirfd/dC0VSKSy776Cu67D667DnbfHWbOhNattS6TZIVEQ0+VV3oVkV21fTs8+CB06AC33Va+7HfbtgoJyRqJehTNzOyK6l5097sjqEckd7z+etireu5cOPJIGD0aumhxZMk+iYKiLtCIqveVEJFE3OGSS+DTT+FvfwsT1lq8T7JUoqD41N1vTVslItlu2zZ44AE480xo0gQmTgwT1nqqWrJcojkK/fNHJFn//Cd07QqXXQaPPhrOtW2rkJCckCgotJeiyM58/DGcdhoce2xYp2nq1DAvIZJDEi0K+GU6CxHJSiNGwLRp4Y6mhQvhJz/RXITknF3eM1skr7nDk0/C4sXh+He/C2sz3XADfO97sZYmEhUFhUiyFi+GPn3g5JPh3nvDuRYtYL/9Yi1LJGoKCpGd2bABrrkGOncOz0bcey+MGhV3VSJpk+j2WBGBMLx0110wZAiMHKld5iTvKChEqjJ/PmzdCr16wZVXQr9+4XuRPKShJ5GKvvgCLroIDjsMhg8P5/bYQyEheU1BIQJh8b6xY8PifePGwcUXw5QpcVclkhE09CQCYbmNCy+Eo44Ki/d17hx3RSIZQ0Eh+evTT8Mtr0VFMHBgGGI66SQ9MCdSiYaeJP98+224k6lDBzj77LCYX716eqpapBoKCskvM2bAIYeErUiLiuCll6CgIO6qRDKahp4kf8yZAyecAO3awTPPwIknxl2RSFZQj0Jy2+bNYQlwgO7dw6T1ggUKCZFdoKCQ3OQOkyfDQQeFUPjss3B+4EBo0CDe2kSyjIJCcs+iRXD88XDqqbDnnvD882GnORGpkUiDwsz6mNliM1tqZiMSXNfDzLab2WlR1iN54PPPoVu3MB8xahS89VZ4NkJEaiyyoDCzusAYoC/QCRhsZp2que5O4PmoapEcV1rKnvPnh++bNoWHHoIlS+CSS8JtryJSK1H2KHoCS919mbt/C0wEBlRx3SXAE8CaCGuRXPXWW3DEERz6y1/Cm2+Gc4MGhcAQkZSI8p9bLYCVFY5XAf+xspqZtQBOBo4GelT3RmY2FBgKUFBwMMXFxamuNStt3Lgxb9ui4OuvaTN+PPs8+yzbCgt577LLWLdhA+Rpe1SUz38uKlNbpEaUQVHVI65e6fheYLi7b7cET8S6+zhgHEDDhod6UVFRikrMbsXFxeRlW5SUhKeqV6yAyy+n/k03sW7evPxsiyrk7Z+LKqgtUiPKoFgFtKpw3BJYXema7sDEspDYG+hnZiXu/mSEdUm2mjMnTFTXqwe//30Iix/8IO6qRHJelHMUbwLtzayNmdUHBgFTK17g7m3cvbW7twYmARcpJOQ7PvkEzjgDevSAxx8P504+WSEhkiaR9SjcvcTMhhHuZqoLTHD3hWZ2QdnrY6P6b0uO2LoV7rkHfv3rMNx0ww3Qv3/cVYnknUjvHXT3acC0SueqDAh3/3mUtUgWGjAgPCzXv38IjAMOiLsikbykJ7MlsyxbBlu2hO+vugqeew6eekohIRIjBYVkhs2b4cYboVOnsFcEwLHHQp8+8dYlIlpmXGLmDpMmwZVXwsqVcOaZcP75cVclIhWoRyHxuvpqOP10+P73YdYseOQR2HffuKsSkQrUo5D0W7cOtm+HJk3grLPC/MPQoVqXSSRDqUch6VNaGhbsO/DAMNQE0LUrXHSRQkIkgykoJD3efBN+9CMYMiRsRXrppXFXJCJJUlBI9CZMgF694OOP4eGH4ZVXwlIcIpIVFBQSjZKS8u1H+/SBa66BxYvh7LMhwQKQIpJ5FBSSejNnhh7D6aeH21/33RfuuAP22CPuykSkBhQUkjqrVoVNg4qKYP16uPzyuCsSkRTQrSaSGrNmQd++4c6mm26C4cNht93irkpEUkBBIbXzxRfheYju3cMzEddeC61bx12ViKSQhp6kZpYuhZNOCnczffMNNGwIf/yjQkIkBykoZNds3AjXXRc2DZo5Ey64AOrWjbsqEYmQhp4kecuXw5FHhh3nzj4b7rwT9tkn7qpEJGIKCtm5DRugcWPYf//wTMSQIeEpaxHJCxp6kup99VVYauOAA+Dzz6FOHRg/XiEhkmcUFPJdpaUhEDp0gDFjwoNzWrRPJG/pb7/8p02boHfvsIjfEUfA6NFhhVcRyVvqUUiwY5/q3XeHww8PGwjNmqWQEBEFRd7btg3uuQdatYL33gvnRo0KW5Jq8T4RQUGR3158MfQYrrgiPFndoEHcFYlIBlJQ5CP3sNzGMceEIaennoLnnoO2beOuTEQykIIin2zbFn41g/bt4dZbw3BT//4aZhKRaiko8oE7TJ0KHTvCjBnh3E03wY03wve+F29tIpLxFBS5bskS6NcPBgwIcxANG8ZdkYhkGQVFLhs5Eg4+GF59Fe6+G95+OzwbISKyC/TAXa5xD1916oR9Is48M2xD2rx53JWJSJZSjyKXvP02HHUUPPhgOB46FB56SCEhIrUSaVCYWR8zW2xmS81sRBWvn2lm75R9vWpmXaKsJ2d9+SVcfDF06waLFoWVXkVEUiSyoSczqwuMAY4DVgFvmtlUd3+vwmXLgaPc/Ssz6wuMA3pFVVNOmjQpbB701VchLG65BfbaK+6qRCSHRDlH0RNY6u7LAMxsIjAA+P+gcPdXK1z/OtAywnpyi3v4dY89wm5zo0fDIYfEW5OI5CTzHR84qX5js9OAPu7+i7Ljs4Fe7j6smuuvAjruuL7Sa0OBoQAFBQcfNmPG6Ehqzgb1v/ySA/74R7buvTfvDh5Mo0aNQmjk+QNzGzduDG0haosK1BblevfuPdfdu9fkZ6PsUVT1yVVlKplZb+B8oMp7N919HGFYioYND/WioqIUlZhFtm0Li/Xdcgts3QrXXsvyRo3Iy7aoQnFxsdqijNqinNoiNaKczF4FtKpw3BJYXfkiMzsEGA8McPcvIqwne82eHYaVrroq7Fm9YAHcfHPcVYlInoiyR/Em0N7M2gCfAIOAMypeYGb7AZOBs919SYS1ZKcdQ0o7us5PPw0nnRRvTSKSdyILCncvMbNhwPNAXWCCuy80swvKXh8L/ApoAvzBwhh7SU3H0HLKli3w29/C8uXw5z9Dp06wcGF4iE5EJM0ifTLb3acB0yqdG1vh+18A35m8zlvu8OSTYX+Ijz4Ke1Vv2wYFBQoJEYmNPn0yxYoV0KcPnHJKGGp68UV4/PEQEiIiMdJaT5lit93CSq/33QcXXQT19L9GRDKDPo3iUloKf/sbPPEETJ4MTZvCBx8oIEQk42joKQ5vvRWW+z7nHFi9GtauDecVEiKSgRQU6bR+fViXqXt3WLoU/vQneP11aNYs7spERKqloEinBg1g5ky49NIwHzFkiO5mEpGMp0+pqL3yCvTvD5s3h6CYPx/uvRcKC2MuTEQkOQqKqKxeDWedFZbcmD8fPvwwnG/QINayRER2lYIi1bZvh7vuggMPDHtF3HBD2Eyoc+e4KxMRqRHdZpNqderAM89A795wzz3Qtm3cFYmI1Ip6FKmwbBkMHhyGm8zg2Wdh6lSFhIjkBAVFbWzeDL/6VVi07+mnw/MRUL7aq4hIDlBQ1NQTT8BBB8Ftt8Gpp8LixVoCXERykuYoamrqVNhrL3jkkXBnk4hIjlKPIllffx2W/543Lxzffz/MmaOQEJGcpx7FzpSWwsMPw4gRsGYNtGoFhx4KjRvHXZmISFooKBKZOxeGDQvrMf3wh+FupsMOi7sqEZG0UlAk8swzYTvSv/wlPGWtdZlEJA/pk6+ikpIw9zCtbPfWa64JdzOdc45CQkTylj79dpg5E7p1g0suCbe+Qth1bs89461LRCRmCopVq8JT1UVF4c6mSZNg/Pi4qxIRyRgKihdfhClTwhPWixaFh+fM4q5KRCRj5Odk9rRp8OWXYYL6rLPCAn6tWsVdlYhIRsqvHsWHH8JPfgInnhgmrd3DJLVCQkSkWvkRFJs2wfXXh8X7iovDfhGzZmmISUQkCfkx9DRvHtx+exhmuvNO2HffuCsSEckauRsUCxbAyy/DhRfCEUfA+++HXedERGSX5N7Q07p1cNll0LUr3HwzbNgQziskRERqJHeCorQUJkyADh3CRPXQofDee1q8T0SklnJn6GnFijDM1KMHjB4dVngVEZFay+4exZo1IRQAWreG2bPDvIRCQkQkZSINCjPrY2aLzWypmY2o4nUzs1Flr79jZt2SeuNt2+C++8Iw0xVXwJIl4XyXLrrlVUQkxSILCjOrC4wB+gKdgMFm1qnSZX2B9mVfQ4EHdva+u5duDD2Gyy+HXr3g3XdDYIiISCSi7FH0BJa6+zJ3/xaYCAyodM0A4GEPXgcKzWyfRG/aYutH4QG6KVNg+nTo2DGS4kVEJIhyMrsFsLLC8SqgVxLXtAA+rXiRmQ0l9DgAttpHHy3g5JNTW2122htYG3cRGUJtUU5tUU5tUa7GzwhEGRRVTRZ4Da7B3ccB4wDMbI67d699edlPbVFObVFObVFObVHOzObU9GejHHpaBVRcba8lsLoG14iISIyiDIo3gfZm1sbM6gODgKmVrpkKnFN299PhwNfu/mnlNxIRkfhENvTk7iVmNgx4HqgLTHD3hWZ2QdnrY4FpQD9gKbAZOC+Jtx4XUcnZSG1RTm1RTm1RTm1RrsZtYe7fmRIQERH5f9n9ZLaIiEROQSEiIgllbFBEtvxHFkqiLc4sa4N3zOxVM+sSR53psLO2qHBdDzPbbmanpbO+dEqmLcysyMzmm9lCM5uZ7hrTJYm/I3ua2dNm9nZZWyQzH5p1zGyCma0xswXVvF6zz013z7gvwuT3h8ABQH3gbaBTpWv6Ac8RnsU4HHgj7rpjbIsfAXuVfd83n9uiwnUvEm6WOC3uumP8c1EIvAfsV3bcLO66Y2yL64A7y75vCnwJ1I+79gja4sdAN2BBNa/X6HMzU3sUkSz/kaV22hbu/qq7f1V2+DrheZRclMyfC4BLgCeANeksLs2SaYszgMnuvgLA3XO1PZJpCwcam5kBjQhBUZLeMqPn7rMIv7fq1OhzM1ODorqlPXb1mlywq7/P8wn/YshFO20LM2sBnAyMTWNdcUjmz0UHYC8zKzazuWZ2TtqqS69k2uJ+4CDCA73vApe5e2l6yssoNfrczNSNi1K2/EcOSPr3aWa9CUFxRKQVxSeZtrgXGO7u2y23l5xPpi3qAYcBxwC7Aa+Z2evuviTq4tIsmbY4AZgPHA20BV4ws5fdfX3EtWWaGn1uZmpQaPmPckn9Ps3sEGA80Nfdv0hTbemWTFt0ByaWhcTeQD8zK3H3J9NSYfok+3dkrbtvAjaZ2SygC5BrQZFMW5wH3OFhoH6pmS0HOgKz01NixqjR52amDj1p+Y9yO20LM9sPmAycnYP/Wqxop23h7m3cvbW7twYmARflYEhAcn9HngKONLN6ZtaQsHrzojTXmQ7JtMUKQs8KM2tOWEl1WVqrzAw1+tzMyB6FR7f8R9ZJsi1+BTQB/lD2L+kSz8EVM5Nsi7yQTFu4+yIzmw68A5QC4929ytsms1mSfy5uA/5sZu8Shl+Gu3vOLT9uZo8BRcDeZrYKuAkogNp9bmoJDxERSShTh55ERCRDKChERCQhBYWIiCSkoBARkYQUFCIikpCCQiRJZavRzq/w1bpsddavzWyemS0ys5vKrq14/n0z+13c9YvUVEY+RyGSoba4e9eKJ8ysNfCyu59kZrsD883smbKXd5zfDZhnZlPc/V/pLVmk9tSjEEmRsqUy5hLWEqp4fgthnaFcXLRS8oCCQiR5u1UYdppS+UUza0JY439hpfN7Ae2BWekpUyS1NPQkkrzvDD2VOdLM5hGWybijbPmIorLz7xDWFbrD3f+dtkpFUkhBIVJ7L7v7SdWdN7MOwCtlcxTz01ybSK1p6EkkYmUr+o4Ehsddi0hNKChE0mMs8GMzaxN3ISK7SqvHiohIQupRiIhIQgoKERFJSEEhIiIJKShERCQhBYWIiCSkoBARkYQUFCIiktD/AQoYz2TK4x/bAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ROC for train set\n",
    "plt.plot(fpr1, tpr1, color = \"Blue\")\n",
    "plt.plot([0,1],[0,1], color = 'Red', linestyle = \"--\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1080a1bb",
   "metadata": {},
   "source": [
    "### The ROC graph for train set is shown as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f493d09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>286</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1\n",
       "True               \n",
       "0          286    3\n",
       "1            0  167"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_confusion1 = pd.DataFrame(confusion_matrix(y_pred00, y_train))\n",
    "df_confusion1 = df_confusion1.rename_axis(index = 'True', columns = 'Predicted')\n",
    "df_confusion1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fc5368",
   "metadata": {},
   "source": [
    "### The confusion marix for train set data is shown as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6b72472",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame()\n",
    "df2 = pd.DataFrame()\n",
    "#dataframe for test \n",
    "df1 = df1.assign(random_state = r, Avg_Acc_Test = accuracy_test, Precision_Test = precision_test,\\\n",
    "                Recall_Test = recall_test, F1_score_Test = f1_score_test, AUC_Test = auc_test)\n",
    "#dataframe for trainf\n",
    "df2 = df2.assign(random_state = r, Avg_Acc_Train = accuracy_train, Precision_Train = precision_train,\\\n",
    "                Recall_Train = recall_train, F1_score_Train = f1_score_train, AUC_Train = auc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bac37c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "random_state       15.500000\n",
       "Avg_Acc_Train       0.990132\n",
       "Precision_Train     0.994240\n",
       "Recall_Train        0.979216\n",
       "F1_score_Train      0.986661\n",
       "AUC_Train           0.998915\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c950a8",
   "metadata": {},
   "source": [
    "### The average accuracy for train set is about 0.990132, precision is about 0.994240, recall is about 0.979216, F1 score is about 0.986661, and AUC is about 0.998915"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1258bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "random_state      15.500000\n",
       "Avg_Acc_Test       0.966077\n",
       "Precision_Test     0.962238\n",
       "Recall_Test        0.946825\n",
       "F1_score_Test      0.953890\n",
       "AUC_Test           0.989850\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453a6b41",
   "metadata": {},
   "source": [
    "### The average accuracy for train set is about 0.966667, precision is about 0.962238, recall is about 0.946825, F1 score is about 0.953890, and AUC is about 0.989850"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b53d1c",
   "metadata": {},
   "source": [
    "## ii. Semi-Supervised Learning/ Self-training: select 50% of the positive class along with 50% of the negative class in the training set as labeled data and the rest as unlabelled data. You can select them randomly.\n",
    "## A. Train an L1-penalized SVM to classify the labeled data Use normalized data. Choose the penalty parameter using 5 fold cross validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39e2152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_para = []\n",
    "distance = []\n",
    "for i in list(range(1,31,1)):\n",
    "    negative = df.loc[df['Diagnosis'] == 'B']\n",
    "    positive = df.loc[df['Diagnosis'] == 'M']\n",
    "\n",
    "    negative = negative.sample(frac=0.2, axis = 'rows', random_state = i)\n",
    "    positive = positive.sample(frac=0.2, axis = 'rows', random_state = i)\n",
    "\n",
    "    test = pd.concat([positive, negative])\n",
    "    train = df.drop(test.index)\n",
    "\n",
    "    negative1 = train.loc[train['Diagnosis'] == 'B']\n",
    "    positive1 = train.loc[train['Diagnosis'] == 'M']\n",
    "\n",
    "    negative1 = negative1.sample(frac=0.5, axis = 'rows', random_state = i)\n",
    "    positive1 = positive1.sample(frac=0.5, axis = 'rows', random_state = i)\n",
    "\n",
    "    labeled = pd.concat([positive1, negative1])\n",
    "    unlabeled = train.drop(labeled.index)\n",
    "\n",
    "    y_unlabel = unlabeled['Diagnosis']\n",
    "    y_label = labeled['Diagnosis']\n",
    "    x_unlabel = unlabeled.loc[:,1:]\n",
    "    x_label = labeled.loc[:,1:]\n",
    "\n",
    "    model1 = svm.LinearSVC(penalty = 'l1', dual = False, max_iter = 300000)\n",
    "    parameters1 = {'C': [0.01, 0.1, 10, 100, 1000, 10000, 100000]}\n",
    "    clf1 = GridSearchCV(model1, parameters1, scoring = 'accuracy', cv = StratifiedKFold(n_splits = 5))\n",
    "    clf1.fit(x_label, y_label)\n",
    "    \n",
    "    distance.append(clf1.decision_function(x_unlabel))\n",
    "    best_para.append(clf1.best_params_['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86f94b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame()\n",
    "df3 = df3.assign(random_state = r, Best_para = best_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ed227dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>random_state</th>\n",
       "      <th>Best_para</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    random_state  Best_para\n",
       "0              1    10000.0\n",
       "1              2       10.0\n",
       "2              3      100.0\n",
       "3              4       10.0\n",
       "4              5   100000.0\n",
       "5              6       10.0\n",
       "6              7     1000.0\n",
       "7              8       10.0\n",
       "8              9        0.1\n",
       "9             10     1000.0\n",
       "10            11       10.0\n",
       "11            12      100.0\n",
       "12            13       10.0\n",
       "13            14    10000.0\n",
       "14            15       10.0\n",
       "15            16       10.0\n",
       "16            17      100.0\n",
       "17            18       10.0\n",
       "18            19       10.0\n",
       "19            20       10.0\n",
       "20            21       10.0\n",
       "21            22    10000.0\n",
       "22            23    10000.0\n",
       "23            24      100.0\n",
       "24            25       10.0\n",
       "25            26       10.0\n",
       "26            27       10.0\n",
       "27            28     1000.0\n",
       "28            29     1000.0\n",
       "29            30      100.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace38a65",
   "metadata": {},
   "source": [
    "### The dataframe shown as above tells us the best penalty parameter in each runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536167ec",
   "metadata": {},
   "source": [
    "## B. Find the unlabeled data point that is the farthest to the decision boundary of the SVM. Let the SVM label it (ignore its true label), and add it to the labeled data, and retrain the SVM. Continue this process until all unlabeled data are used. Test the final SVM on the test data and the average accuracy, precision, recall, F1-score, and AUC, for both training and test sets over your M runs. Plot the ROC and report the confusion matrix for training and testing in one of the runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03202103",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_para = []\n",
    "distance = []\n",
    "\n",
    "accuracy_train1 = []\n",
    "precision_train1 = []\n",
    "recall_train1 = []\n",
    "f1_score_train1 = []\n",
    "auc_train1 = []\n",
    "\n",
    "accuracy_test1 = []\n",
    "precision_test1 = []\n",
    "recall_test1 = []\n",
    "f1_score_test1 = []\n",
    "auc_test1 = []\n",
    "\n",
    "r = []\n",
    "for i in list(range(1,31,1)):\n",
    "    df = df.replace([\"B\", \"M\"], [0, 1])\n",
    "    \n",
    "    negative = df.loc[df['Diagnosis'] == 0]\n",
    "    positive = df.loc[df['Diagnosis'] == 1]\n",
    "\n",
    "    negative = negative.sample(frac=0.2, axis = 'rows', random_state = i)\n",
    "    positive = positive.sample(frac=0.2, axis = 'rows', random_state = i)\n",
    "\n",
    "    test = pd.concat([positive, negative])\n",
    "    train = df.drop(test.index)\n",
    "    \n",
    "    y_test = test['Diagnosis']\n",
    "    y_train = train['Diagnosis']\n",
    "    x_test = test.loc[:,1:]\n",
    "    x_train = train.loc[:,1:]\n",
    "\n",
    "    negative1 = train.loc[train['Diagnosis'] == 0]\n",
    "    positive1 = train.loc[train['Diagnosis'] == 1]\n",
    "\n",
    "    negative1 = negative1.sample(frac=0.5, axis = 'rows', random_state = i)\n",
    "    positive1 = positive1.sample(frac=0.5, axis = 'rows', random_state = i)\n",
    "    \n",
    "    labeled = pd.concat([positive1, negative1])\n",
    "    unlabeled = train.drop(labeled.index)\n",
    "\n",
    "    y_unlabel = unlabeled['Diagnosis']\n",
    "    y_label = labeled['Diagnosis']\n",
    "    x_unlabel = unlabeled.loc[:,1:]\n",
    "    x_label = labeled.loc[:,1:]\n",
    "\n",
    "\n",
    "    model1 = svm.LinearSVC(C = df3['Best_para'][i-1], penalty = 'l1', dual = False, max_iter = 300000)\n",
    "#    parameters1 = np.array(df3['Best_para'][i])\n",
    "#    clf2 = GridSearchCV(model1, parameters1, scoring = 'accuracy', cv = StratifiedKFold(n_splits = 5))\n",
    "    \n",
    "    for t in (x_unlabel.index):\n",
    "        model1.fit(x_label, y_label)\n",
    "        l = abs(np.argmax(model1.decision_function(x_unlabel)))\n",
    "        x_unlabel = x_unlabel.drop(x_unlabel.iloc[l:l+1].index)\n",
    "        labeled = pd.concat([labeled,x_unlabel.iloc[l:l+1]])\n",
    "    \n",
    "    \n",
    "    model1.fit(x_train, y_train)\n",
    "    y_pred1 = model1.predict(x_test)\n",
    "    \n",
    "    #for AUC (test)\n",
    "    y_score1 = model1.decision_function(x_test)\n",
    "    fpr2, tpr2, thresholds2 = roc_curve(y_test, y_score1)\n",
    "    auc3 = auc(fpr2, tpr2)\n",
    "\n",
    "    accuracy_test1.append(model1.score(x_test, y_test))\n",
    "    precision_test1.append(precision_score(y_test, y_pred1))\n",
    "    recall_test1.append(recall_score(y_test, y_pred1))\n",
    "    f1_score_test1.append(f1_score(y_test, y_pred1))\n",
    "    auc_test1.append(auc3)\n",
    "    \n",
    "    #for train set\n",
    "    y_pred2 = model1.predict(x_train)\n",
    "    #for AUC (train)\n",
    "    y_score2 = model1.decision_function(x_train)\n",
    "    fpr3, tpr3, thresholds3 = roc_curve(y_train, y_score2)\n",
    "    auc4 = auc(fpr3, tpr3)\n",
    "    \n",
    "#     #for train set\n",
    "#     y_pred2 = model1.predict(x_train)\n",
    "#     #for AUC (train)\n",
    "#     y_score2 = model1.decision_function(x_train)\n",
    "#     fpr3, tpr3, thresholds3 = roc_curve(y_train, y_score2)\n",
    "#     auc4 = auc(fpr3, tpr3)\n",
    "    \n",
    "    \n",
    "    accuracy_train1.append(model1.score(x_train, y_train))\n",
    "    precision_train1.append(precision_score(y_train, y_pred2))\n",
    "    recall_train1.append(recall_score(y_train, y_pred2))\n",
    "    f1_score_train1.append(f1_score(y_train, y_pred2))\n",
    "    auc_train1.append(auc4)\n",
    "    r.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c585077b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ROC graph for train set\n",
    "plt.plot(fpr2, tpr2, color = \"Blue\")\n",
    "plt.plot([0,1],[0,1], color = 'Red', linestyle = \"--\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6309cf",
   "metadata": {},
   "source": [
    "### The ROC graph for test set is shown as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4353eaf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0   1\n",
       "True             \n",
       "0          70   4\n",
       "1           1  38"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_confusion2 = pd.DataFrame(confusion_matrix(y_pred1, y_test))\n",
    "df_confusion2 = df_confusion2.rename_axis(index = 'True', columns = 'Predicted')\n",
    "df_confusion2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce917314",
   "metadata": {},
   "source": [
    "### The confusion marix for test set data is shown as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef2cbceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ROC graph for train set\n",
    "plt.plot(fpr3, tpr3, color = \"Blue\")\n",
    "plt.plot([0,1],[0,1], color = 'Red', linestyle = \"--\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fc5e51",
   "metadata": {},
   "source": [
    "### The ROC graph for train set is shown as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e20113a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>286</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1\n",
       "True               \n",
       "0          286    2\n",
       "1            0  168"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_confusion3 = pd.DataFrame(confusion_matrix(y_pred2, y_train))\n",
    "df_confusion3 = df_confusion3.rename_axis(index = 'True', columns = 'Predicted')\n",
    "df_confusion3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3cf013",
   "metadata": {},
   "source": [
    "### The confusion marix for train set data is shown as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ae70c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame()\n",
    "df5 = pd.DataFrame()\n",
    "#dataframe for test \n",
    "df4 = df4.assign(random_state = r, Avg_Acc_Test = accuracy_test1, Precision_Test = precision_test1,\\\n",
    "                Recall_Test = recall_test1, F1_score_Test = f1_score_test1, AUC_Test = auc_test1)\n",
    "#dataframe for trainf\n",
    "df5 = df5.assign(random_state = r, Avg_Acc_Train = accuracy_train1, Precision_Train = precision_train1,\\\n",
    "                Recall_Train = recall_train1, F1_score_Train = f1_score_train1, AUC_Train = auc_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb6ab1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "random_state      15.500000\n",
       "Avg_Acc_Test       0.958997\n",
       "Precision_Test     0.951652\n",
       "Recall_Test        0.938889\n",
       "F1_score_Test      0.944183\n",
       "AUC_Test           0.984630\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1369c4c9",
   "metadata": {},
   "source": [
    "### The average accuracy for test set is about 0.958997, precision is about 0.951652, recall is about 0.938889, F1 score is about 0.944183, and AUC is about 0.984630"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0345d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "random_state       15.500000\n",
       "Avg_Acc_Train       0.992836\n",
       "Precision_Train     0.996408\n",
       "Recall_Train        0.984314\n",
       "F1_score_Train      0.990261\n",
       "AUC_Train           0.999026\n",
       "dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28118c56",
   "metadata": {},
   "source": [
    "### The average accuracy for train set is about 0.992836, precision is about 0.996408, recall is about 0.984314, F1 score is about 0.990261, and AUC is about 0.999026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e57f2e",
   "metadata": {},
   "source": [
    "## iii. Unsupervised Learning: Run k-means algorithm on the whole training set. Ignore the labels of the data, and assume k = 2. \n",
    "## A. Run the k-means algorithm multiple times. Make sure that you initialize the algoritm randomly. How do you make sure that the algorithm was not trapped in a local minimum?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7f9712",
   "metadata": {},
   "source": [
    "## B. Compute the centers of the two clusters and find the closest 30 data points to each center. Read the true labels of those 30 data points and take a majority poll within them. The majority poll becomes the label predicted by k-means for the members of each cluster. Then compare the labels provided by k-means with the true labels of the training data and report the average accuracy, precision, recall, F1-score, and AUC over M runs, and ROC and the confusion matrix for one of the runs.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "524b9507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e661988",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_label_train = []\n",
    "precision_label_train = []\n",
    "recall_label_train = []\n",
    "f1_score_label_train = []\n",
    "auc_label_train = []\n",
    "r = []\n",
    "\n",
    "accuracy_label = []\n",
    "precision_label = []\n",
    "recall_label = []\n",
    "f1_score_label = []\n",
    "auc_label = []\n",
    "\n",
    "for i in list(range(1,31,1)):\n",
    "\n",
    "    negative = df.loc[df['Diagnosis'] == 0]\n",
    "    positive = df.loc[df['Diagnosis'] == 1]\n",
    "\n",
    "    negative = negative.sample(frac=0.2, axis = 'rows', random_state = i)\n",
    "    positive = positive.sample(frac=0.2, axis = 'rows', random_state = i)\n",
    "\n",
    "    test = pd.concat([positive, negative])\n",
    "    train = df.drop(test.index)\n",
    "\n",
    "    y_test = test['Diagnosis']\n",
    "    y_train = train['Diagnosis']\n",
    "    x_test = test.loc[:,1:]\n",
    "    x_train = train.loc[:,1:]\n",
    "\n",
    "    km = KMeans(n_clusters = 2, init = 'random').fit(x_train)\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors = 30, algorithm = 'ball_tree').fit(x_train)\n",
    "    distance, indices = nbrs.kneighbors([km.cluster_centers_[0], km.cluster_centers_[1]])\n",
    "    \n",
    "    nbrs1 = NearestNeighbors(n_neighbors = len(x_train), algorithm = 'ball_tree').fit(x_train)\n",
    "    distance1, indices1 = nbrs1.kneighbors([km.cluster_centers_[0], km.cluster_centers_[1]])\n",
    "    \n",
    "    x_train['klabel'] = km.labels_\n",
    "    condition = [(x_train['klabel'] == 0), (x_train['klabel'] == 1 )]\n",
    "    value = [y_train.iloc[indices[0]].mode()[0], y_train.iloc[indices[1]].mode()[0]]\n",
    "    x_train['label'] = np.select(condition, value)\n",
    "\n",
    "    #for AUC (train)\n",
    "    dc0 = 1 - softmax(distance1[0])\n",
    "    dc1 = 1 - softmax(distance1[1])\n",
    "    dc2 = dc0/(dc0+dc1)\n",
    "    df10 = pd.DataFrame()\n",
    "    df10 = df10.assign(DC0 = dc0, DC1 = dc1, D_to_c0 = dc2)\n",
    "    df10['label'] = km.labels_\n",
    "    df10['klabel'] = np.select(condition, value)\n",
    "    \n",
    "    condition1 = [(df10['label'] == df10['klabel']), (df10['label'] != df10['klabel'])]\n",
    "    value1 = [dc1, dc0]\n",
    "    df10['D_to_t1'] = np.select(condition1, value1)\n",
    "    y_pred5 = km.labels_\n",
    "    \n",
    "    fpr4, tpr4, thresholds4 = roc_curve(y_train, df10['D_to_t1'])\n",
    "    auc5 = auc(fpr4, tpr4)\n",
    "    \n",
    "    accuracy_label_train.append(accuracy_score(y_train, x_train['label']))\n",
    "    precision_label_train.append(precision_score(y_train, x_train['label']))\n",
    "    recall_label_train.append(recall_score(y_train, x_train['label']))\n",
    "    f1_score_label_train.append(f1_score(y_train, x_train['label']))\n",
    "    auc_label_train.append(auc5)\n",
    "    r.append(i)\n",
    "    \n",
    "    x_train = x_train.drop(columns = ['klabel', 'label'])\n",
    "    \n",
    "    #for Test\n",
    "    km = KMeans(n_clusters = 2, init = 'random').fit(x_train)\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors = 30, algorithm = 'ball_tree').fit(x_test)\n",
    "    distance, indices = nbrs.kneighbors([km.cluster_centers_[0], km.cluster_centers_[1]])\n",
    "    \n",
    "    nbrs1 = NearestNeighbors(n_neighbors = len(x_test), algorithm = 'ball_tree').fit(x_test)\n",
    "    distance1, indices1 = nbrs1.kneighbors([km.cluster_centers_[0], km.cluster_centers_[1]])\n",
    "    \n",
    "    temp = km.predict(x_test)\n",
    "    x_test['klabel'] = temp\n",
    "    condition = [(x_test['klabel'] == 0), (x_test['klabel'] == 1 )]\n",
    "    value = [y_test.iloc[indices[0]].mode()[0], y_test.iloc[indices[1]].mode()[0]]\n",
    "    x_test['label'] = np.select(condition, value)\n",
    "    y_pred6 = temp\n",
    "\n",
    "    #for AUC (test)\n",
    "    dc0 = 1 - softmax(distance1[0])\n",
    "    dc1 = 1 - softmax(distance1[1])\n",
    "    dc2 = dc0/(dc0+dc1)\n",
    "    df10 = pd.DataFrame()\n",
    "    df10 = df10.assign(DC0 = dc0, DC1 = dc1, D_to_c0 = dc2, label = temp, klabel = np.select(condition, value))\n",
    "    \n",
    "    condition1 = [(df10['label'] == df10['klabel']), (df10['label'] != df10['klabel'])]\n",
    "    value1 = [dc1, dc0]\n",
    "    df10['D_to_t1'] = np.select(condition1, value1)\n",
    "    \n",
    "    fpr5, tpr5, thresholds5 = roc_curve(y_test, df10['D_to_t1'])\n",
    "    auc6 = auc(fpr5, tpr5)\n",
    "\n",
    "    accuracy_label.append(accuracy_score(y_test, x_test['label']))\n",
    "    precision_label.append(precision_score(y_test, x_test['label']))\n",
    "    recall_label.append(recall_score(y_test, x_test['label']))\n",
    "    f1_score_label.append(f1_score(y_test, x_test['label']))\n",
    "    auc_label.append(auc6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "47838b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ROC for train\n",
    "plt.plot(fpr4, tpr4, color = \"Blue\")\n",
    "plt.plot([0,1],[0,1], color = 'Red', linestyle = \"--\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab2cf79",
   "metadata": {},
   "source": [
    "### The ROC graph for my train set is shown as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c0a8f8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>278</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1\n",
       "True               \n",
       "0          278   27\n",
       "1            8  143"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_confusion4 = pd.DataFrame(confusion_matrix(y_pred5, y_train))\n",
    "df_confusion4 = df_confusion4.rename_axis(index = 'True', columns = 'Predicted')\n",
    "df_confusion4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9b4979",
   "metadata": {},
   "source": [
    "### The confusion marix for train set data is shown as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5b740d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ROC for test\n",
    "plt.plot(fpr5, tpr5, color = \"Blue\")\n",
    "plt.plot([0,1],[0,1], color = 'Red', linestyle = \"--\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a51854",
   "metadata": {},
   "source": [
    "### The ROC graph for my test set is shown as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "17003192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0   1\n",
       "True             \n",
       "0           3  37\n",
       "1          68   5"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_confusion5 = pd.DataFrame(confusion_matrix(y_pred6, y_test))\n",
    "df_confusion5 = df_confusion5.rename_axis(index = 'True', columns = 'Predicted')\n",
    "df_confusion5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a211433",
   "metadata": {},
   "source": [
    "### The confusion marix for test set data is shown as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "66343204",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = pd.DataFrame()\n",
    "df11 = pd.DataFrame()\n",
    "\n",
    "#dataframe for label\n",
    "df6 = df6.assign(random_state = r, Avg_Acc_train = accuracy_label_train, Precision_train = precision_label_train,\\\n",
    "                Recall_train = recall_label_train, F1_score_train = f1_score_label_train, auc_train = auc_label_train)\n",
    "\n",
    "df11 = df11.assign(random_state = r, Avg_Acc_test = accuracy_label, Precision_test = precision_label,\\\n",
    "                Recall_test = recall_label, F1_score_test = f1_score_label, auc_test = auc_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c2fb314e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "random_state       15.500000\n",
       "Avg_Acc_train       0.929094\n",
       "Precision_train     0.956228\n",
       "Recall_train        0.848824\n",
       "F1_score_train      0.899231\n",
       "auc_train           0.673455\n",
       "dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa52000",
   "metadata": {},
   "source": [
    "### The average accuracy for train set is about 0.929094, precision is about 0.956228, recall is about 0.848824, F1 score is about 0.899231, and AUC is about 0.673455"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4d4510d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "random_state      15.500000\n",
       "Avg_Acc_test       0.926254\n",
       "Precision_test     0.950513\n",
       "Recall_test        0.846032\n",
       "F1_score_test      0.894373\n",
       "auc_test           1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df11.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c80eb1",
   "metadata": {},
   "source": [
    "### The average accuracy for test set is about 0.926254, precision is about 0.950513, recall is about 0.846032, F1 score is about 0.894373, and AUC is about 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b60e0f",
   "metadata": {},
   "source": [
    "## C. Classify test data based on their proximity to the centers of the clusters. Report the average accuracy, precision, recall, F1-score, and AUC over M runs, and ROC and the confusion matrix for one of the runs for the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b28093",
   "metadata": {},
   "source": [
    "## iv. Spectral Clustering: Repeat 1(b)iii using spectral clustering, which is clustering based on kernels.3 Research what spectral clustering is. Use RBF kernel with gamma=1 or find a gamma for which the two clutsres have the same balance as the one in original data set (if the positive class has p and the negative class has n samples, the two clusters must have p and n members). Do not label data based on their proximity to cluster center, because spectral clustering may give you non-convex clusters . Instead, use fit  predict method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "db8b81bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2a773714",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_label2 = []\n",
    "precision_label2 = []\n",
    "recall_label2 = []\n",
    "f1_score_label2 = []\n",
    "auc_label2 = []\n",
    "r = []\n",
    "\n",
    "accuracy_label_train2 = []\n",
    "precision_label_train2 = []\n",
    "recall_label_train2 = []\n",
    "f1_score_label_train2 = []\n",
    "auc_label_train2 = []\n",
    "\n",
    "threshold = [0, 0.001, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "\n",
    "for i in list(range(1,31,1)):\n",
    "    negative = df.loc[df['Diagnosis'] == 0]\n",
    "    positive = df.loc[df['Diagnosis'] == 1]\n",
    "\n",
    "    negative = negative.sample(frac=0.2, axis = 'rows', random_state = i)\n",
    "    positive = positive.sample(frac=0.2, axis = 'rows', random_state = i)\n",
    "\n",
    "    test = pd.concat([positive, negative])\n",
    "    train = df.drop(test.index)\n",
    "\n",
    "    y_test = test['Diagnosis']\n",
    "    y_train = train['Diagnosis']\n",
    "    x_test = test.loc[:,1:]\n",
    "    x_train = train.loc[:,1:]\n",
    "\n",
    "    spec = SpectralClustering(n_clusters = 2, gamma = 1, assign_labels = 'discretize', affinity = 'rbf').fit(x_train)\n",
    "\n",
    "    y_pred3 = spec.fit_predict(x_test)\n",
    "    y_pred4 = spec.fit_predict(x_train)\n",
    "    \n",
    "    df001 = pd.DataFrame(y_test.tolist(), columns = ['Diagnosis'])\n",
    "    df002 = pd.DataFrame(y_pred3.tolist(), columns = ['Diagnosis_1'])\n",
    "    df001 = pd.concat([df001,df002], axis = 1)\n",
    "    \n",
    "    df010 = df001.groupby(['Diagnosis_1'])[['Diagnosis']].agg(lambda x:x.value_counts().index[0])\n",
    "    \n",
    "    x_test['true_label'] = y_pred3\n",
    "    condition = [(x_test['true_label'] == 0), (x_test['true_label'] == 1 )]\n",
    "    value = [df010['Diagnosis'][0], df010['Diagnosis'][1]]\n",
    "    x_test['label'] = np.select(condition, value)\n",
    "    \n",
    "    n1 = len(x_test[x_test.label == 1])\n",
    "    n0 = len(x_test[x_test.label == 0])\n",
    "    \n",
    "\n",
    "    accuracy_label2.append(accuracy_score(y_test, x_test['label']))\n",
    "    precision_label2.append(precision_score(y_test, x_test['label']))\n",
    "    recall_label2.append(recall_score(y_test, x_test['label']))\n",
    "    f1_score_label2.append(f1_score(y_test, x_test['label']))\n",
    "    r.append(i)\n",
    "    \n",
    "    \n",
    "    df001 = pd.DataFrame(y_train.tolist(), columns = ['Diagnosis'])\n",
    "    df002 = pd.DataFrame(y_pred4.tolist(), columns = ['Diagnosis_1'])\n",
    "    df001 = pd.concat([df001,df002], axis = 1)\n",
    "    \n",
    "    df010 = df001.groupby(['Diagnosis_1'])[['Diagnosis']].agg(lambda x:x.value_counts().index[0])\n",
    "    \n",
    "    x_train['true_label'] = y_pred4\n",
    "    condition = [(x_train['true_label'] == 0), (x_train['true_label'] == 1 )]\n",
    "    value = [df010['Diagnosis'][0], df010['Diagnosis'][1]]\n",
    "    x_train['label'] = np.select(condition, value)\n",
    "    \n",
    "    \n",
    "    accuracy_label_train2.append(accuracy_score(y_train, x_train['label']))\n",
    "    precision_label_train2.append(precision_score(y_train, x_train['label']))\n",
    "    recall_label_train2.append(recall_score(y_train, x_train['label']))\n",
    "    f1_score_label_train2.append(f1_score(y_train, x_train['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d631e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = pd.DataFrame()\n",
    "df9 = pd.DataFrame()\n",
    "\n",
    "#dataframe for label\n",
    "df7 = df7.assign(random_state = r, Avg_Acc_Test = accuracy_label2, Precision_Test = precision_label2,\\\n",
    "                Recall_Test = recall_label2, F1_score_Test = f1_score_label2)\n",
    "\n",
    "#dataframe for label\n",
    "df9 = df9.assign(random_state = r, Avg_Acc_train = accuracy_label_train2, Precision_train = precision_label_train2,\\\n",
    "                Recall_train = recall_label_train2, F1_score_train = f1_score_label_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e09ae940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0   1\n",
       "True             \n",
       "0           0  24\n",
       "1          71  18"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_confusion6 = pd.DataFrame(confusion_matrix(y_pred3, y_test))\n",
    "df_confusion6 = df_confusion6.rename_axis(index = 'True', columns = 'Predicted')\n",
    "df_confusion6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5fe804",
   "metadata": {},
   "source": [
    "### The confusion marix for test set data is shown as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f625b167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>283</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1\n",
       "True               \n",
       "0          283   36\n",
       "1            3  134"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_confusion7 = pd.DataFrame(confusion_matrix(y_pred4, y_train))\n",
    "df_confusion7 = df_confusion7.rename_axis(index = 'True', columns = 'Predicted')\n",
    "df_confusion7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff9c8e1",
   "metadata": {},
   "source": [
    "### The confusion marix for train set data is shown as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "33fd5ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "random_state      15.500000\n",
       "Avg_Acc_Test       0.904425\n",
       "Precision_Test     0.968320\n",
       "Recall_Test        0.771429\n",
       "F1_score_Test      0.842844\n",
       "dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1306a3ae",
   "metadata": {},
   "source": [
    "### The average accuracy for test set is about 0.904425, precision is about 0.968320, recall is about 0.771429, F1 score is about 0.842844, and AUC is about __________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e802ccc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "random_state       15.500000\n",
       "Avg_Acc_train       0.918275\n",
       "Precision_train     0.982398\n",
       "Recall_train        0.795098\n",
       "F1_score_train      0.878667\n",
       "dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df9.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6670b7b4",
   "metadata": {},
   "source": [
    "### The average accuracy for train set is about 0.918275, precision is about 0.982398, recall is about 0.795098, F1 score is about 0.878667, and AUC is about ______"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa89060e",
   "metadata": {},
   "source": [
    "## v. One can expect that supervised learning on the full data set works better than semi-supervised learning with half of the data set labeled.One can expect that unsupervised learning underperforms in such situations. Compare the results you obtained by those methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518ec3ae",
   "metadata": {},
   "source": [
    "### Compares to all average accuracy for test set, my semi-supervised learning gives me the highest avg accuracy score. As well as for to all the average accuracy for my train set, my semi-supervised leanring gives me the highest avg accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e52eafb",
   "metadata": {},
   "source": [
    "# 2. Active Learning Using Support Vector Machines\n",
    "## (a) Download the banknote authentication Data Set from: https://archive.ics. uci.edu/ml/datasets/banknote+authentication. Choose 472 data points randomly as the test set, and the remaining 900 points as the training set. This is a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d1cb6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f0f70177",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/data_banknote_authentication.txt', \"r\") as fp:\n",
    "    f = fp.readlines()[0:1372]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "336caf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(0,len(f),1):\n",
    "    data.append(f[i].split(\",\"))\n",
    "    \n",
    "#remover the \\n\n",
    "for i, s in enumerate(data):\n",
    "    data[i] = list(map(lambda x:x.strip(), s))\n",
    "    \n",
    "df8 = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d1ae24",
   "metadata": {},
   "source": [
    "## (b) Repeat each of the following two procedures 50 times. You will have 50 errors for 90 SVMs per each procedure.\n",
    "## i. Traina SVM with a pool of 10 randomly selected data points from the training set using linear kernel and L1 penalty. Select the penalty parameter using 5-fold cross validation.4 Repeat this process by adding 10 other randomly selected data points to the pool, until you use all the 900 points. Do NOT replace the samples back into the training set at each step. Calculate the test error for each SVM. You will have 90 SVMs that were trained using 10, 20, 30, ... , 900 data points and their 90 test errors. You have implemented passive learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e57b8acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "db72c58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e82af452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "x = df8.iloc[:,:4]\n",
    "y = df8.iloc[:,4:]\n",
    "df111 = pd.DataFrame()\n",
    "\n",
    "for i in list(range(1,51,1)):\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 472/1372, random_state = 1, stratify = y)\n",
    "    \n",
    "    x_train2_2 = pd.DataFrame()\n",
    "    y_train2_2 = pd.DataFrame()\n",
    "    score = []\n",
    "    test_error = []\n",
    "    params = []\n",
    "    validation = []\n",
    "    for k in list(range(1,91,1)):\n",
    "        \n",
    "\n",
    "        if k <= 89:\n",
    "            \n",
    "            x_train2, x_test2, y_train2, y_test2 = train_test_split(x_train, y_train, \\\n",
    "                                                                    test_size = (len(x_train)-10)/len(x_train), \\\n",
    "                                                                    random_state = i, stratify = y_train)\n",
    "\n",
    "            x_train = x_train.drop(x_train2.index)\n",
    "            y_train = y_train.drop(y_train2.index)\n",
    "            x_train2_2 = pd.concat([x_train2_2,x_train2])\n",
    "            y_train2_2 = pd.concat([y_train2_2,y_train2])\n",
    "\n",
    "\n",
    "            model = svm.LinearSVC(penalty = 'l1', dual = False, max_iter = 500000, tol = 0.5)\n",
    "            parameters = {'C':[0.01, 10, 100, 1000, 10000]}\n",
    "            clf = GridSearchCV(model, parameters, scoring = 'accuracy', cv = StratifiedKFold(n_splits = 5))\n",
    "            clf.fit(x_train2_2, np.ravel(y_train2_2))\n",
    "\n",
    "\n",
    "            validation.append(clf.best_score_)\n",
    "            test_error.append(1 - clf.score(x_test, np.ravel(y_test)))\n",
    "            params.append(clf.best_params_['C'])\n",
    "            \n",
    "        else:\n",
    "            clf.fit(x_train, np.ravel(y_train))\n",
    "\n",
    "            validation.append(clf.best_score_)\n",
    "            test_error.append(1 - clf.score(x_test, np.ravel(y_test)))\n",
    "            params.append(clf.best_params_['C'])\n",
    "                \n",
    "                         \n",
    "    \n",
    "    df111['c_' + str(i)] = params\n",
    "    df111['Test_error' + str(i)] = test_error\n",
    "    df111['Validation' + str(i)] = validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "75d2c06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_1</th>\n",
       "      <th>Test_error1</th>\n",
       "      <th>Validation1</th>\n",
       "      <th>c_2</th>\n",
       "      <th>Test_error2</th>\n",
       "      <th>Validation2</th>\n",
       "      <th>c_3</th>\n",
       "      <th>Test_error3</th>\n",
       "      <th>Validation3</th>\n",
       "      <th>c_4</th>\n",
       "      <th>...</th>\n",
       "      <th>Validation47</th>\n",
       "      <th>c_48</th>\n",
       "      <th>Test_error48</th>\n",
       "      <th>Validation48</th>\n",
       "      <th>c_49</th>\n",
       "      <th>Test_error49</th>\n",
       "      <th>Validation49</th>\n",
       "      <th>c_50</th>\n",
       "      <th>Test_error50</th>\n",
       "      <th>Validation50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.266949</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.116525</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.127119</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.112288</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.116525</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>100</td>\n",
       "      <td>0.122881</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>10</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>100</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>10</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.974419</td>\n",
       "      <td>100</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.970930</td>\n",
       "      <td>100</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>10000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>100</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.974419</td>\n",
       "      <td>10</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.973256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>10</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.978161</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.981609</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.977011</td>\n",
       "      <td>10000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977011</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.974713</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.977011</td>\n",
       "      <td>10</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.975862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.976136</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.981818</td>\n",
       "      <td>10</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.976136</td>\n",
       "      <td>10000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>100</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.972727</td>\n",
       "      <td>10</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.986364</td>\n",
       "      <td>100</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.977273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.977528</td>\n",
       "      <td>100</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.974157</td>\n",
       "      <td>100</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.975281</td>\n",
       "      <td>10000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.982022</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.975281</td>\n",
       "      <td>10</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.974157</td>\n",
       "      <td>100</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.970787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>10</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.148305</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.158898</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      c_1  Test_error1  Validation1    c_2  Test_error2  Validation2    c_3  \\\n",
       "0      10     0.048729     0.900000    100     0.266949     0.900000     10   \n",
       "1      10     0.036017     0.850000   1000     0.127119     0.950000  10000   \n",
       "2      10     0.057203     0.966667  10000     0.088983     0.966667    100   \n",
       "3     100     0.031780     0.975000     10     0.008475     0.925000    100   \n",
       "4      10     0.036017     0.960000     10     0.046610     0.980000    100   \n",
       "..    ...          ...          ...    ...          ...          ...    ...   \n",
       "85     10     0.027542     0.974419    100     0.025424     0.970930    100   \n",
       "86     10     0.023305     0.978161   1000     0.019068     0.981609  10000   \n",
       "87  10000     0.027542     0.976136   1000     0.014831     0.981818     10   \n",
       "88   1000     0.025424     0.977528    100     0.023305     0.974157    100   \n",
       "89     10     0.055085     0.800000    100     0.025424     1.000000     10   \n",
       "\n",
       "    Test_error3  Validation3    c_4  ...  Validation47  c_48  Test_error48  \\\n",
       "0      0.161017     0.900000     10  ...      1.000000    10      0.016949   \n",
       "1      0.112288     0.950000     10  ...      0.950000    10      0.036017   \n",
       "2      0.122881     1.000000  10000  ...      0.966667    10      0.012712   \n",
       "3      0.052966     0.950000     10  ...      0.925000    10      0.031780   \n",
       "4      0.046610     0.960000     10  ...      0.940000   100      0.033898   \n",
       "..          ...          ...    ...  ...           ...   ...           ...   \n",
       "85     0.025424     0.976744  10000  ...      0.976744   100      0.019068   \n",
       "86     0.019068     0.977011  10000  ...      0.977011  1000      0.014831   \n",
       "87     0.033898     0.976136  10000  ...      0.977273   100      0.023305   \n",
       "88     0.021186     0.975281  10000  ...      0.982022  1000      0.027542   \n",
       "89     0.148305     0.900000     10  ...      0.900000    10      0.088983   \n",
       "\n",
       "    Validation48   c_49  Test_error49  Validation49  c_50  Test_error50  \\\n",
       "0       0.800000     10      0.116525      0.900000    10      0.031780   \n",
       "1       0.900000     10      0.116525      1.000000    10      0.033898   \n",
       "2       0.966667    100      0.019068      1.000000   100      0.050847   \n",
       "3       0.975000    100      0.023305      1.000000    10      0.057203   \n",
       "4       0.980000     10      0.019068      1.000000    10      0.082627   \n",
       "..           ...    ...           ...           ...   ...           ...   \n",
       "85      0.976744  10000      0.014831      0.974419    10      0.025424   \n",
       "86      0.974713  10000      0.021186      0.977011    10      0.023305   \n",
       "87      0.972727     10      0.027542      0.986364   100      0.021186   \n",
       "88      0.975281     10      0.014831      0.974157   100      0.019068   \n",
       "89      0.900000    100      0.158898      1.000000   100      0.025424   \n",
       "\n",
       "    Validation50  \n",
       "0       1.000000  \n",
       "1       0.950000  \n",
       "2       0.966667  \n",
       "3       1.000000  \n",
       "4       0.940000  \n",
       "..           ...  \n",
       "85      0.973256  \n",
       "86      0.975862  \n",
       "87      0.977273  \n",
       "88      0.970787  \n",
       "89      1.000000  \n",
       "\n",
       "[90 rows x 150 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "29a3b8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     227.942916\n",
       "1     309.541888\n",
       "2     373.272566\n",
       "3     190.866243\n",
       "4     376.801731\n",
       "         ...    \n",
       "85    988.266483\n",
       "86    865.865989\n",
       "87    850.865428\n",
       "88    661.132936\n",
       "89    175.875243\n",
       "Length: 90, dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df111.mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9860899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "colum_name = [c for c in df111 if c.startswith('Test_error')]\n",
    "df110 = df111[colum_name]\n",
    "df110.insert(0,'mean50',df110.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "41d1d0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean50</th>\n",
       "      <th>Test_error1</th>\n",
       "      <th>Test_error2</th>\n",
       "      <th>Test_error3</th>\n",
       "      <th>Test_error4</th>\n",
       "      <th>Test_error5</th>\n",
       "      <th>Test_error6</th>\n",
       "      <th>Test_error7</th>\n",
       "      <th>Test_error8</th>\n",
       "      <th>Test_error9</th>\n",
       "      <th>...</th>\n",
       "      <th>Test_error41</th>\n",
       "      <th>Test_error42</th>\n",
       "      <th>Test_error43</th>\n",
       "      <th>Test_error44</th>\n",
       "      <th>Test_error45</th>\n",
       "      <th>Test_error46</th>\n",
       "      <th>Test_error47</th>\n",
       "      <th>Test_error48</th>\n",
       "      <th>Test_error49</th>\n",
       "      <th>Test_error50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.128347</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.266949</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.099576</td>\n",
       "      <td>0.362288</td>\n",
       "      <td>0.141949</td>\n",
       "      <td>0.224576</td>\n",
       "      <td>0.078390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171610</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.165254</td>\n",
       "      <td>0.243644</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.230932</td>\n",
       "      <td>0.148305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.116525</td>\n",
       "      <td>0.031780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.078263</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.127119</td>\n",
       "      <td>0.112288</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.362288</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.099576</td>\n",
       "      <td>0.078390</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.116525</td>\n",
       "      <td>0.033898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.054364</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.122881</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.156780</td>\n",
       "      <td>0.129237</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.050847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.036229</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.057203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.036992</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.082627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.023008</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.025424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.023305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.019534</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.021186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.022161</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.113729</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.148305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.110169</td>\n",
       "      <td>0.307203</td>\n",
       "      <td>0.097458</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184322</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.182203</td>\n",
       "      <td>0.199153</td>\n",
       "      <td>0.207627</td>\n",
       "      <td>0.141949</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.158898</td>\n",
       "      <td>0.025424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean50  Test_error1  Test_error2  Test_error3  Test_error4  Test_error5  \\\n",
       "0   0.128347     0.048729     0.266949     0.161017     0.029661     0.099576   \n",
       "1   0.078263     0.036017     0.127119     0.112288     0.042373     0.021186   \n",
       "2   0.054364     0.057203     0.088983     0.122881     0.027542     0.025424   \n",
       "3   0.036229     0.031780     0.008475     0.052966     0.021186     0.019068   \n",
       "4   0.036992     0.036017     0.046610     0.046610     0.023305     0.021186   \n",
       "..       ...          ...          ...          ...          ...          ...   \n",
       "85  0.023008     0.027542     0.025424     0.025424     0.036017     0.021186   \n",
       "86  0.021483     0.023305     0.019068     0.019068     0.019068     0.021186   \n",
       "87  0.019534     0.027542     0.014831     0.033898     0.031780     0.010593   \n",
       "88  0.022161     0.025424     0.023305     0.021186     0.023305     0.010593   \n",
       "89  0.113729     0.055085     0.025424     0.148305     0.021186     0.110169   \n",
       "\n",
       "    Test_error6  Test_error7  Test_error8  Test_error9  ...  Test_error41  \\\n",
       "0      0.362288     0.141949     0.224576     0.078390  ...      0.171610   \n",
       "1      0.362288     0.046610     0.050847     0.036017  ...      0.021186   \n",
       "2      0.021186     0.055085     0.023305     0.050847  ...      0.014831   \n",
       "3      0.025424     0.027542     0.029661     0.040254  ...      0.019068   \n",
       "4      0.048729     0.021186     0.048729     0.101695  ...      0.012712   \n",
       "..          ...          ...          ...          ...  ...           ...   \n",
       "85     0.027542     0.023305     0.021186     0.025424  ...      0.025424   \n",
       "86     0.019068     0.027542     0.019068     0.014831  ...      0.016949   \n",
       "87     0.014831     0.021186     0.021186     0.014831  ...      0.010593   \n",
       "88     0.023305     0.010593     0.016949     0.023305  ...      0.023305   \n",
       "89     0.307203     0.097458     0.036017     0.031780  ...      0.184322   \n",
       "\n",
       "    Test_error42  Test_error43  Test_error44  Test_error45  Test_error46  \\\n",
       "0       0.021186      0.165254      0.243644      0.084746      0.230932   \n",
       "1       0.029661      0.099576      0.078390      0.029661      0.080508   \n",
       "2       0.019068      0.156780      0.129237      0.027542      0.019068   \n",
       "3       0.021186      0.050847      0.019068      0.016949      0.033898   \n",
       "4       0.023305      0.046610      0.031780      0.019068      0.019068   \n",
       "..           ...           ...           ...           ...           ...   \n",
       "85      0.023305      0.021186      0.033898      0.014831      0.019068   \n",
       "86      0.016949      0.021186      0.014831      0.023305      0.010593   \n",
       "87      0.016949      0.014831      0.012712      0.016949      0.014831   \n",
       "88      0.023305      0.023305      0.019068      0.012712      0.027542   \n",
       "89      0.069915      0.055085      0.182203      0.199153      0.207627   \n",
       "\n",
       "    Test_error47  Test_error48  Test_error49  Test_error50  \n",
       "0       0.148305      0.016949      0.116525      0.031780  \n",
       "1       0.042373      0.036017      0.116525      0.033898  \n",
       "2       0.021186      0.012712      0.019068      0.050847  \n",
       "3       0.033898      0.031780      0.023305      0.057203  \n",
       "4       0.025424      0.033898      0.019068      0.082627  \n",
       "..           ...           ...           ...           ...  \n",
       "85      0.027542      0.019068      0.014831      0.025424  \n",
       "86      0.023305      0.014831      0.021186      0.023305  \n",
       "87      0.012712      0.023305      0.027542      0.021186  \n",
       "88      0.014831      0.027542      0.014831      0.019068  \n",
       "89      0.141949      0.088983      0.158898      0.025424  \n",
       "\n",
       "[90 rows x 51 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df110"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29738f9",
   "metadata": {},
   "source": [
    "## ii. selected data points to the pool, until you use all the 900 points. Do NOT replace the samples back into the training set at each step. Calculate the test error for each SVM. You will have 90 SVMs that were trained using 10, 20, 30, ... , 900 data points and their 90 test errors. You have implemented passive learning. Train a SVM with a pool of 10 randomly selected data points from the training set 5 using linear kernel and L1 penalty. Select the parameters of the SVM with 5-fold cross validation. Choose the 10 closest data points in the training set to the hyperplane of the SVM6 and add them to the pool. Do not replace the samples back into the training set. Train a new SVM using the pool. Repeat this process until all training data is used. You will have 90 SVMs that were trained using 10, 20, 30,..., 900 data points and their 90 test errors. You have implemented active learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b80e2983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "x = df8.iloc[:,:4]\n",
    "y = df8.iloc[:,4:]\n",
    "df000 = pd.DataFrame()\n",
    "\n",
    "for i in list(range(1,51,1)):\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 472/1372, random_state = 1, stratify = y)\n",
    "    x_train2_2 = pd.DataFrame()\n",
    "    y_train2_2 = pd.DataFrame()\n",
    "    \n",
    "    score = []\n",
    "    test_error = []\n",
    "    params = []\n",
    "    validation1 = []\n",
    "    for k in list(range(1,91,1)):\n",
    "        \n",
    "        x_train2, x_test2, y_train2, y_test2 = train_test_split(x_train, y_train, \\\n",
    "                                                                            test_size = (len(x_train)-10)/len(x_train), \\\n",
    "                                                                            random_state = i, stratify = y_train)\n",
    "          \n",
    "        \n",
    "        model1.fit(x_train2, np.ravel(y_train2))\n",
    "#        b = abs(model1.decision_function(x_train2))\n",
    "#        x_train2['distance'] = b\n",
    "        l = abs(model1.decision_function(x_test2))\n",
    "        x_test2['distance'] = l\n",
    "        x_test2 = x_test2.sort_values(by ='distance')\n",
    "        \n",
    "        x_train2_2 = pd.concat([x_train2_2, x_test2.iloc[:10]])\n",
    "        y_train2_2 = pd.concat([y_train2_2, y_test2.iloc[:10]])\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "#         x_train = x_train.drop(x_train2.index)\n",
    "#         y_train = y_train.drop(y_train2.index)\n",
    "#         x_train2_2 = pd.DataFrame()\n",
    "#         y_train2_2 = pd.DataFrame()\n",
    "#         x_train2_2 = pd.concat([x_train2_2,x_train2])\n",
    "#         y_train2_2 = pd.concat([y_train2_2,y_train2])\n",
    "\n",
    "\n",
    "        model = svm.LinearSVC(penalty = 'l1', dual = False, max_iter = 500000, tol = 0.5)\n",
    "        parameters = {'C':[0.01, 10, 100, 1000, 10000]}\n",
    "        clf = GridSearchCV(model, parameters, scoring = 'accuracy', cv = StratifiedKFold(n_splits = 5), n_jobs = 3)\n",
    "        x_train2_2 = x_train2_2.drop(columns = ['distance'])\n",
    "        clf.fit(x_train2_2, np.ravel(y_train2_2))\n",
    "\n",
    "\n",
    "        validation1.append(clf.best_score_)\n",
    "        test_error.append(1 - clf.score(x_test, np.ravel(y_test)))\n",
    "        params.append(clf.best_params_['C'])\n",
    "\n",
    "    else:\n",
    "        clf.fit(x_train2_2, np.ravel(y_train2_2))\n",
    "        \n",
    "        validation1.append(clf.best_score_)\n",
    "        test_error.append(1 - clf.score(x_test, np.ravel(y_test)))\n",
    "        params.append(clf.best_params_['C'])\n",
    "            \n",
    "                         \n",
    "    df000['c_' + str(i)] = params\n",
    "    df000['Test_error' + str(i)] = test_error\n",
    "    df000['Validation' + str(i)] = validation1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "66b2a5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c_1               79.237692\n",
       "Test_error1        0.589239\n",
       "Validation1        0.697811\n",
       "c_2              695.172967\n",
       "Test_error2        0.308367\n",
       "                   ...     \n",
       "Test_error49       0.616456\n",
       "Validation49       0.599843\n",
       "c_50            1444.945165\n",
       "Test_error50       0.328110\n",
       "Validation50       0.642048\n",
       "Length: 150, dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df000.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5d4d98b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_1</th>\n",
       "      <th>Test_error1</th>\n",
       "      <th>Validation1</th>\n",
       "      <th>c_2</th>\n",
       "      <th>Test_error2</th>\n",
       "      <th>Validation2</th>\n",
       "      <th>c_3</th>\n",
       "      <th>Test_error3</th>\n",
       "      <th>Validation3</th>\n",
       "      <th>c_4</th>\n",
       "      <th>...</th>\n",
       "      <th>Validation47</th>\n",
       "      <th>c_48</th>\n",
       "      <th>Test_error48</th>\n",
       "      <th>Validation48</th>\n",
       "      <th>c_49</th>\n",
       "      <th>Test_error49</th>\n",
       "      <th>Validation49</th>\n",
       "      <th>c_50</th>\n",
       "      <th>Test_error50</th>\n",
       "      <th>Validation50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.288136</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.699153</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.451271</td>\n",
       "      <td>0.7</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.665254</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.00</td>\n",
       "      <td>0.230932</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.190678</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.182203</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.256356</td>\n",
       "      <td>0.8</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.658898</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.209746</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.00</td>\n",
       "      <td>0.561441</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.402542</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.8</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.699153</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.199153</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.00</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.197034</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.324153</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.586864</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.713983</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.201271</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.00</td>\n",
       "      <td>0.667373</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.311441</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.8</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.616525</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.398305</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.294492</td>\n",
       "      <td>0.619540</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.599576</td>\n",
       "      <td>0.541379</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.596552</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.8</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.567797</td>\n",
       "      <td>0.597701</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.411017</td>\n",
       "      <td>0.640230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.559322</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.290254</td>\n",
       "      <td>0.619318</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.703390</td>\n",
       "      <td>0.539773</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.592045</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.8</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>0.559322</td>\n",
       "      <td>0.601136</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.224576</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.281780</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.756356</td>\n",
       "      <td>0.542697</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.593258</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.8</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.580508</td>\n",
       "      <td>0.601124</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.218220</td>\n",
       "      <td>0.631461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.279661</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.800847</td>\n",
       "      <td>0.531111</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.598889</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.222458</td>\n",
       "      <td>0.631111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.283898</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.783898</td>\n",
       "      <td>0.532222</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595556</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.8</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.572034</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.453390</td>\n",
       "      <td>0.631111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        c_1  Test_error1  Validation1      c_2  Test_error2  Validation2  \\\n",
       "0   1000.00     0.288136     0.700000     0.01     0.444915     0.500000   \n",
       "1     10.00     0.230932     0.700000  1000.00     0.190678     0.550000   \n",
       "2     10.00     0.561441     0.633333    10.00     0.186441     0.533333   \n",
       "3    100.00     0.555085     0.675000   100.00     0.197034     0.550000   \n",
       "4     10.00     0.667373     0.680000    10.00     0.173729     0.580000   \n",
       "..      ...          ...          ...      ...          ...          ...   \n",
       "86     0.01     0.555085     0.700000     0.01     0.294492     0.619540   \n",
       "87     0.01     0.559322     0.700000     0.01     0.290254     0.619318   \n",
       "88     0.01     0.555085     0.700000     0.01     0.281780     0.600000   \n",
       "89     0.01     0.555085     0.700000     0.01     0.279661     0.600000   \n",
       "90     0.01     0.555085     0.700000     0.01     0.283898     0.600000   \n",
       "\n",
       "        c_3  Test_error3  Validation3       c_4  ...  Validation47    c_48  \\\n",
       "0      10.0     0.699153     0.700000      0.01  ...      0.500000  100.00   \n",
       "1     100.0     0.182203     0.600000      0.01  ...      0.500000   10.00   \n",
       "2    1000.0     0.402542     0.600000  10000.00  ...      0.500000   10.00   \n",
       "3     100.0     0.324153     0.600000      0.01  ...      0.550000   10.00   \n",
       "4      10.0     0.311441     0.520000      0.01  ...      0.560000   10.00   \n",
       "..      ...          ...          ...       ...  ...           ...     ...   \n",
       "86  10000.0     0.599576     0.541379      0.01  ...      0.596552    0.01   \n",
       "87    100.0     0.703390     0.539773     10.00  ...      0.592045    0.01   \n",
       "88     10.0     0.756356     0.542697      0.01  ...      0.593258    0.01   \n",
       "89   1000.0     0.800847     0.531111      0.01  ...      0.616667    0.01   \n",
       "90     10.0     0.783898     0.532222      0.01  ...      0.595556    0.01   \n",
       "\n",
       "    Test_error48  Validation48      c_49  Test_error49  Validation49     c_50  \\\n",
       "0       0.451271           0.7     10.00      0.665254      0.700000    100.0   \n",
       "1       0.256356           0.8     10.00      0.658898      0.550000     10.0   \n",
       "2       0.555085           0.8     10.00      0.699153      0.633333  10000.0   \n",
       "3       0.586864           0.8   1000.00      0.713983      0.650000     10.0   \n",
       "4       0.555085           0.8     10.00      0.616525      0.580000  10000.0   \n",
       "..           ...           ...       ...           ...           ...      ...   \n",
       "86      0.555085           0.8     10.00      0.567797      0.597701   1000.0   \n",
       "87      0.555085           0.8  10000.00      0.559322      0.601136     10.0   \n",
       "88      0.555085           0.8     10.00      0.580508      0.601124    100.0   \n",
       "89      0.555085           0.8      0.01      0.555085      0.598889    100.0   \n",
       "90      0.555085           0.8     10.00      0.572034      0.600000   1000.0   \n",
       "\n",
       "    Test_error50  Validation50  \n",
       "0       0.305085      0.700000  \n",
       "1       0.209746      0.700000  \n",
       "2       0.199153      0.700000  \n",
       "3       0.201271      0.750000  \n",
       "4       0.398305      0.660000  \n",
       "..           ...           ...  \n",
       "86      0.411017      0.640230  \n",
       "87      0.224576      0.625000  \n",
       "88      0.218220      0.631461  \n",
       "89      0.222458      0.631111  \n",
       "90      0.453390      0.631111  \n",
       "\n",
       "[91 rows x 150 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eaad0c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "colum_name = [c for c in df000 if c.startswith('Test_error')]\n",
    "df100 = df000[colum_name]\n",
    "df100.insert(0,'mean50',df100.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "662e7418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean50</th>\n",
       "      <th>Test_error1</th>\n",
       "      <th>Test_error2</th>\n",
       "      <th>Test_error3</th>\n",
       "      <th>Test_error4</th>\n",
       "      <th>Test_error5</th>\n",
       "      <th>Test_error6</th>\n",
       "      <th>Test_error7</th>\n",
       "      <th>Test_error8</th>\n",
       "      <th>Test_error9</th>\n",
       "      <th>...</th>\n",
       "      <th>Test_error41</th>\n",
       "      <th>Test_error42</th>\n",
       "      <th>Test_error43</th>\n",
       "      <th>Test_error44</th>\n",
       "      <th>Test_error45</th>\n",
       "      <th>Test_error46</th>\n",
       "      <th>Test_error47</th>\n",
       "      <th>Test_error48</th>\n",
       "      <th>Test_error49</th>\n",
       "      <th>Test_error50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.460127</td>\n",
       "      <td>0.288136</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.699153</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.406780</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.588983</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.722458</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.633475</td>\n",
       "      <td>0.622881</td>\n",
       "      <td>0.273305</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.451271</td>\n",
       "      <td>0.665254</td>\n",
       "      <td>0.305085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.230932</td>\n",
       "      <td>0.190678</td>\n",
       "      <td>0.182203</td>\n",
       "      <td>0.362288</td>\n",
       "      <td>0.430085</td>\n",
       "      <td>0.644068</td>\n",
       "      <td>0.574153</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.288136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.728814</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.563559</td>\n",
       "      <td>0.618644</td>\n",
       "      <td>0.478814</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.256356</td>\n",
       "      <td>0.658898</td>\n",
       "      <td>0.209746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.457881</td>\n",
       "      <td>0.561441</td>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.402542</td>\n",
       "      <td>0.317797</td>\n",
       "      <td>0.391949</td>\n",
       "      <td>0.656780</td>\n",
       "      <td>0.512712</td>\n",
       "      <td>0.447034</td>\n",
       "      <td>0.222458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.747881</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.252119</td>\n",
       "      <td>0.614407</td>\n",
       "      <td>0.364407</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.699153</td>\n",
       "      <td>0.199153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.477585</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.197034</td>\n",
       "      <td>0.324153</td>\n",
       "      <td>0.362288</td>\n",
       "      <td>0.436441</td>\n",
       "      <td>0.690678</td>\n",
       "      <td>0.567797</td>\n",
       "      <td>0.447034</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.775424</td>\n",
       "      <td>0.362288</td>\n",
       "      <td>0.387712</td>\n",
       "      <td>0.646186</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.667373</td>\n",
       "      <td>0.586864</td>\n",
       "      <td>0.713983</td>\n",
       "      <td>0.201271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.457119</td>\n",
       "      <td>0.667373</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.311441</td>\n",
       "      <td>0.362288</td>\n",
       "      <td>0.385593</td>\n",
       "      <td>0.705508</td>\n",
       "      <td>0.608051</td>\n",
       "      <td>0.654661</td>\n",
       "      <td>0.233051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.682203</td>\n",
       "      <td>0.362288</td>\n",
       "      <td>0.298729</td>\n",
       "      <td>0.408898</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.675847</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.616525</td>\n",
       "      <td>0.398305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.517373</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.294492</td>\n",
       "      <td>0.599576</td>\n",
       "      <td>0.213983</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>0.580508</td>\n",
       "      <td>0.646186</td>\n",
       "      <td>0.641949</td>\n",
       "      <td>0.243644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.737288</td>\n",
       "      <td>0.489407</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.622881</td>\n",
       "      <td>0.730932</td>\n",
       "      <td>0.434322</td>\n",
       "      <td>0.648305</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.567797</td>\n",
       "      <td>0.411017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.501483</td>\n",
       "      <td>0.559322</td>\n",
       "      <td>0.290254</td>\n",
       "      <td>0.703390</td>\n",
       "      <td>0.177966</td>\n",
       "      <td>0.419492</td>\n",
       "      <td>0.576271</td>\n",
       "      <td>0.557203</td>\n",
       "      <td>0.618644</td>\n",
       "      <td>0.400424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.652542</td>\n",
       "      <td>0.436441</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.597458</td>\n",
       "      <td>0.896186</td>\n",
       "      <td>0.434322</td>\n",
       "      <td>0.616525</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.559322</td>\n",
       "      <td>0.224576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.509068</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.281780</td>\n",
       "      <td>0.756356</td>\n",
       "      <td>0.213983</td>\n",
       "      <td>0.413136</td>\n",
       "      <td>0.588983</td>\n",
       "      <td>0.646186</td>\n",
       "      <td>0.608051</td>\n",
       "      <td>0.468220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.730932</td>\n",
       "      <td>0.495763</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.614407</td>\n",
       "      <td>0.680085</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.599576</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.580508</td>\n",
       "      <td>0.218220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.494534</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.279661</td>\n",
       "      <td>0.800847</td>\n",
       "      <td>0.188559</td>\n",
       "      <td>0.411017</td>\n",
       "      <td>0.684322</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>0.523305</td>\n",
       "      <td>0.309322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.735169</td>\n",
       "      <td>0.406780</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.620763</td>\n",
       "      <td>0.603814</td>\n",
       "      <td>0.447034</td>\n",
       "      <td>0.593220</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.222458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.519831</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.283898</td>\n",
       "      <td>0.783898</td>\n",
       "      <td>0.283898</td>\n",
       "      <td>0.415254</td>\n",
       "      <td>0.697034</td>\n",
       "      <td>0.582627</td>\n",
       "      <td>0.644068</td>\n",
       "      <td>0.218220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.756356</td>\n",
       "      <td>0.506356</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.618644</td>\n",
       "      <td>0.582627</td>\n",
       "      <td>0.447034</td>\n",
       "      <td>0.641949</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.572034</td>\n",
       "      <td>0.453390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean50  Test_error1  Test_error2  Test_error3  Test_error4  Test_error5  \\\n",
       "0   0.460127     0.288136     0.444915     0.699153     0.444915     0.406780   \n",
       "1   0.460000     0.230932     0.190678     0.182203     0.362288     0.430085   \n",
       "2   0.457881     0.561441     0.186441     0.402542     0.317797     0.391949   \n",
       "3   0.477585     0.555085     0.197034     0.324153     0.362288     0.436441   \n",
       "4   0.457119     0.667373     0.173729     0.311441     0.362288     0.385593   \n",
       "..       ...          ...          ...          ...          ...          ...   \n",
       "86  0.517373     0.555085     0.294492     0.599576     0.213983     0.423729   \n",
       "87  0.501483     0.559322     0.290254     0.703390     0.177966     0.419492   \n",
       "88  0.509068     0.555085     0.281780     0.756356     0.213983     0.413136   \n",
       "89  0.494534     0.555085     0.279661     0.800847     0.188559     0.411017   \n",
       "90  0.519831     0.555085     0.283898     0.783898     0.283898     0.415254   \n",
       "\n",
       "    Test_error6  Test_error7  Test_error8  Test_error9  ...  Test_error41  \\\n",
       "0      0.444915     0.588983     0.444915     0.444915  ...      0.722458   \n",
       "1      0.644068     0.574153     0.444915     0.288136  ...      0.728814   \n",
       "2      0.656780     0.512712     0.447034     0.222458  ...      0.747881   \n",
       "3      0.690678     0.567797     0.447034     0.250000  ...      0.775424   \n",
       "4      0.705508     0.608051     0.654661     0.233051  ...      0.682203   \n",
       "..          ...          ...          ...          ...  ...           ...   \n",
       "86     0.580508     0.646186     0.641949     0.243644  ...      0.737288   \n",
       "87     0.576271     0.557203     0.618644     0.400424  ...      0.652542   \n",
       "88     0.588983     0.646186     0.608051     0.468220  ...      0.730932   \n",
       "89     0.684322     0.610169     0.523305     0.309322  ...      0.735169   \n",
       "90     0.697034     0.582627     0.644068     0.218220  ...      0.756356   \n",
       "\n",
       "    Test_error42  Test_error43  Test_error44  Test_error45  Test_error46  \\\n",
       "0       0.444915      0.633475      0.622881      0.273305      0.444915   \n",
       "1       0.444915      0.563559      0.618644      0.478814      0.444915   \n",
       "2       0.444915      0.252119      0.614407      0.364407      0.444915   \n",
       "3       0.362288      0.387712      0.646186      0.694915      0.444915   \n",
       "4       0.362288      0.298729      0.408898      0.813559      0.444915   \n",
       "..           ...           ...           ...           ...           ...   \n",
       "86      0.489407      0.555085      0.622881      0.730932      0.434322   \n",
       "87      0.436441      0.555085      0.597458      0.896186      0.434322   \n",
       "88      0.495763      0.555085      0.614407      0.680085      0.444915   \n",
       "89      0.406780      0.555085      0.620763      0.603814      0.447034   \n",
       "90      0.506356      0.555085      0.618644      0.582627      0.447034   \n",
       "\n",
       "    Test_error47  Test_error48  Test_error49  Test_error50  \n",
       "0       0.444915      0.451271      0.665254      0.305085  \n",
       "1       0.444915      0.256356      0.658898      0.209746  \n",
       "2       0.444915      0.555085      0.699153      0.199153  \n",
       "3       0.667373      0.586864      0.713983      0.201271  \n",
       "4       0.675847      0.555085      0.616525      0.398305  \n",
       "..           ...           ...           ...           ...  \n",
       "86      0.648305      0.555085      0.567797      0.411017  \n",
       "87      0.616525      0.555085      0.559322      0.224576  \n",
       "88      0.599576      0.555085      0.580508      0.218220  \n",
       "89      0.593220      0.555085      0.555085      0.222458  \n",
       "90      0.641949      0.555085      0.572034      0.453390  \n",
       "\n",
       "[91 rows x 51 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c76950",
   "metadata": {},
   "source": [
    "## (c) Average the 50 test errors for each of the incrementally trained 90 SVMs in 2(b)i and 2(b)ii. By doing so, you are performing a Monte Carlo simulation. Plot average test error versus number of training instances for both active and passive learners on the same figure and report your conclusions. Here, you are actually obtaining a learning curve by Monte-Carlo simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f9558b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb952daf880>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(df100.index*10, df100['mean50'], label = \"Passive Learning\")\n",
    "plt.plot(df110.index*10, df110['mean50'], label = \"Active Learning\")\n",
    "plt.xlabel('Number of xs')\n",
    "plt.ylabel('Average test error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae9d4c0",
   "metadata": {},
   "source": [
    "# Reference:\n",
    "    https://stackoverflow.com/questions/48641632/extracting-specific-columns-from-pandas-dataframe\n",
    "    \n",
    "    https://stackoverflow.com/questions/21660937/get-nearest-point-to-centroid-scikit-learn\n",
    "    \n",
    "    https://stackoverflow.com/questions/53790832/how-to-get-n-numbers-of-data-points-which-are-nearest-from-a-clusters-center\n",
    "    \n",
    "    https://stackoverflow.com/questions/36195457/how-to-get-the-samples-in-each-cluster\n",
    "    \n",
    "    https://stackoverflow.com/questions/55749114/how-to-do-a-majority-voting-on-columns-in-pandas\n",
    "    \n",
    "    https://stackoverflow.com/questions/27275236/pandas-best-way-to-select-all-columns-whose-names-start-with-x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
